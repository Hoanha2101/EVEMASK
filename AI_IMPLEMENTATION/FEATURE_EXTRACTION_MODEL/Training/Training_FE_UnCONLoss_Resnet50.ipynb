{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9642830,"sourceType":"datasetVersion","datasetId":5888595},{"sourceId":12252246,"sourceType":"datasetVersion","datasetId":7719970}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Install required libraries</h1>","metadata":{}},{"cell_type":"code","source":"!pip install albumentations pytorch-metric-learning --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:10:50.113964Z","iopub.execute_input":"2025-06-27T06:10:50.114648Z","iopub.status.idle":"2025-06-27T06:10:53.332759Z","shell.execute_reply.started":"2025-06-27T06:10:50.114619Z","shell.execute_reply":"2025-06-27T06:10:53.331871Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"<h1>Library</h1>","metadata":{}},{"cell_type":"code","source":"import time\nimport os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torchvision import models\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.neighbors import KNeighborsClassifier\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:22:07.743107Z","iopub.execute_input":"2025-06-27T06:22:07.743449Z","iopub.status.idle":"2025-06-27T06:22:07.748903Z","shell.execute_reply.started":"2025-06-27T06:22:07.743423Z","shell.execute_reply":"2025-06-27T06:22:07.748306Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"<h1>Device</h1>","metadata":{}},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:10:54.805695Z","iopub.execute_input":"2025-06-27T06:10:54.805973Z","iopub.status.idle":"2025-06-27T06:10:54.909478Z","shell.execute_reply.started":"2025-06-27T06:10:54.805950Z","shell.execute_reply":"2025-06-27T06:10:54.908894Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nMemory: 14.74 GB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"<h1>Hyperparameters </h1>","metadata":{}},{"cell_type":"code","source":"# Hyperparameters \ntrain_dir = \"/kaggle/input/data-augmented-model-2/augmented_data_model_2\" \ntest_dir = \"/kaggle/input/logo-verify-test/logo_verify_test\"   \nINPUT_SIZE = 224\nBATCH_SIZE = 64\nEMBEDDING_DIM = 256\nMARGIN = 1.0  # Margin for contrastive loss\nLR = 3e-4\nWEIGHT_DECAY = 1e-4\nNUM_EPOCHS = 50\nACCUM_STEPS = 2\nKEY_IMAGE = \"000000\"  # Reference image identifier\n\nprint(f\"Training directory: {train_dir}\")\nprint(f\"Test directory: {test_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:13:22.515976Z","iopub.execute_input":"2025-06-27T06:13:22.516520Z","iopub.status.idle":"2025-06-27T06:13:22.521218Z","shell.execute_reply.started":"2025-06-27T06:13:22.516492Z","shell.execute_reply":"2025-06-27T06:13:22.520575Z"}},"outputs":[{"name":"stdout","text":"Training directory: /kaggle/input/data-augmented-model-2/augmented_data_model_2\nTest directory: /kaggle/input/logo-verify-test/logo_verify_test\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"<h1>Show check data and Visualization</h1>","metadata":{}},{"cell_type":"code","source":"# # Kiểm tra cấu trúc thư mục\n# def analyze_dataset(directory):\n#     class_names = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n#     num_classes = len(class_names)\n    \n#     class_stats = {}\n#     total_images = 0\n#     min_images = float('inf')\n#     max_images = 0\n    \n#     for cls in class_names:\n#         cls_dir = os.path.join(directory, cls)\n#         num_images = len([f for f in os.listdir(cls_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))])\n#         class_stats[cls] = num_images\n#         total_images += num_images\n#         min_images = min(min_images, num_images)\n#         max_images = max(max_images, num_images)\n    \n#     print(f\"Dataset Analysis: {directory}\")\n#     print(f\"Number of classes: {num_classes}\")\n#     print(f\"Total images: {total_images}\")\n#     print(f\"Min images per class: {min_images}\")\n#     print(f\"Max images per class: {max_images}\")\n#     print(f\"Average images per class: {total_images/num_classes:.2f}\")\n    \n#     # Vẽ biểu đồ phân bố\n#     plt.figure(figsize=(12, 6))\n#     plt.bar(class_stats.keys(), class_stats.values())\n#     plt.xticks(rotation=90)\n#     plt.title(\"Class Distribution\")\n#     plt.ylabel(\"Number of Images\")\n#     plt.tight_layout()\n#     plt.show()\n    \n#     return class_stats\n\n# train_stats = analyze_dataset(train_dir)\n# test_stats = analyze_dataset(test_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:13:31.552835Z","iopub.execute_input":"2025-06-27T06:13:31.553471Z","iopub.status.idle":"2025-06-27T06:13:31.557027Z","shell.execute_reply.started":"2025-06-27T06:13:31.553448Z","shell.execute_reply":"2025-06-27T06:13:31.556318Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"<h1>Transform</h1>","metadata":{}},{"cell_type":"code","source":"def get_transforms():\n    train_transform = A.Compose([\n        A.Resize(INPUT_SIZE, INPUT_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.3),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n        A.OneOf([\n            A.GaussNoise(var_limit=(10.0, 50.0)),\n            A.GaussianBlur(blur_limit=(3, 5)),\n        ], p=0.3),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    \n    test_transform = A.Compose([\n        A.Resize(INPUT_SIZE, INPUT_SIZE),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    \n    return train_transform, test_transform\n\ntrain_transform, test_transform = get_transforms()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:13:46.582297Z","iopub.execute_input":"2025-06-27T06:13:46.582612Z","iopub.status.idle":"2025-06-27T06:13:46.599041Z","shell.execute_reply.started":"2025-06-27T06:13:46.582587Z","shell.execute_reply":"2025-06-27T06:13:46.598372Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"<h2>Check data Augment</h2>","metadata":{}},{"cell_type":"code","source":"# # Load một ảnh mẫu để kiểm tra transform\n# sample_img_path = next(iter(train_stats.keys())) + '/' + os.listdir(os.path.join(train_dir, next(iter(train_stats.keys()))))[0]\n# sample_img = cv2.imread(os.path.join(train_dir, sample_img_path))\n# sample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n\n# # Áp dụng transform\n# transformed = train_transform(image=sample_img)[\"image\"]\n\n# # Hiển thị kết quả\n# plt.figure(figsize=(12, 6))\n# plt.subplot(1, 2, 1)\n# plt.imshow(sample_img)\n# plt.title(\"Original Image\")\n# plt.axis('off')\n\n# plt.subplot(1, 2, 2)\n# plt.imshow(unnormalize(transformed))\n# plt.title(\"Augmented Image\")\n# plt.axis('off')\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:13:55.602387Z","iopub.execute_input":"2025-06-27T06:13:55.602660Z","iopub.status.idle":"2025-06-27T06:13:55.606066Z","shell.execute_reply.started":"2025-06-27T06:13:55.602642Z","shell.execute_reply":"2025-06-27T06:13:55.605500Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"<h1>DataLoader</h1>","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport time\nimport cv2\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\n\nclass ImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.samples = []\n        self.class_to_idx = {}\n        self.idx_to_class = {}\n        self.label_to_indices = defaultdict(list)\n        self._build_dataset()\n        \n    def _build_dataset(self):\n        # Find classes\n        class_names = sorted([d for d in os.listdir(self.root_dir) \n                             if os.path.isdir(os.path.join(self.root_dir, d))])\n        \n        # Create class mapping\n        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}\n        \n        # Build samples list\n        for cls_name in class_names:\n            cls_dir = os.path.join(self.root_dir, cls_name)\n            for fname in os.listdir(cls_dir):\n                if fname.lower().endswith(('.jpg','.jpeg','.png')):\n                    img_path = os.path.join(cls_dir, fname)\n                    label = self.class_to_idx[cls_name]\n                    self.samples.append((img_path, label))\n                    self.label_to_indices[label].append(len(self.samples)-1)\n        \n        print(f\"Loaded {len(self.samples)} images from {len(class_names)} classes\")\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        \n        try:\n            img = cv2.imread(img_path)\n            if img is None:\n                raise Exception(\"OpenCV failed to load image\")\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        except:\n            img = np.array(Image.open(img_path).convert('RGB'))\n        \n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n            \n        return img, label, img_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:14:14.267377Z","iopub.execute_input":"2025-06-27T06:14:14.267646Z","iopub.status.idle":"2025-06-27T06:14:14.276980Z","shell.execute_reply.started":"2025-06-27T06:14:14.267627Z","shell.execute_reply":"2025-06-27T06:14:14.276374Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"<h1>BACBONE</h1>","metadata":{}},{"cell_type":"markdown","source":"<h2>RESNET 50</h2>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nclass Network(nn.Module):\n    def __init__(self, emb_dim=128):\n        super(Network, self).__init__()\n\n        base_model = models.resnet50(pretrained=True)\n\n        # Bỏ layer cuối cùng (fc)\n        self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # Output: [B, 2048, 1, 1]\n\n        # FC Head\n        self.fc = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.PReLU(),\n            nn.Linear(512, emb_dim)\n        )\n\n    def forward(self, x):\n        x = self.backbone(x)           # [B, 2048, 1, 1]\n        x = torch.flatten(x, 1)        # [B, 2048]\n        x = self.fc(x)                 # [B, emb_dim]\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:14:28.959597Z","iopub.execute_input":"2025-06-27T06:14:28.960377Z","iopub.status.idle":"2025-06-27T06:14:28.964959Z","shell.execute_reply.started":"2025-06-27T06:14:28.960329Z","shell.execute_reply":"2025-06-27T06:14:28.964133Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"<h2>RESNET 18</h2>","metadata":{}},{"cell_type":"code","source":"# class Network(nn.Module):\n#     def __init__(self, emb_dim=256):\n#         super(Network, self).__init__()\n#         base_model = models.resnet18(pretrained=True)\n        \n#         # Remove final classification layer\n#         self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n        \n#         # Embedding head\n#         self.fc = nn.Sequential(\n#             nn.Linear(512, 512),\n#             nn.BatchNorm1d(512),\n#             nn.ReLU(),\n#             nn.Linear(512, emb_dim)\n#         )\n        \n#     def forward(self, x):\n#         x = self.backbone(x)\n#         x = torch.flatten(x, 1)\n#         x = self.fc(x)\n#         return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:17:10.475255Z","iopub.execute_input":"2025-06-27T06:17:10.475566Z","iopub.status.idle":"2025-06-27T06:17:10.481206Z","shell.execute_reply.started":"2025-06-27T06:17:10.475544Z","shell.execute_reply":"2025-06-27T06:17:10.480649Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"<h1>Contrastive Loss</h1>","metadata":{}},{"cell_type":"code","source":"class ContrastiveLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n        \n    def forward(self, output1, output2, label):\n        # Euclidean distance\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        \n        # Contrastive loss\n        loss_contrastive = torch.mean(\n            (1 - label) * torch.pow(euclidean_distance, 2) +\n            label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n        )\n        \n        return loss_contrastive","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:17:12.325427Z","iopub.execute_input":"2025-06-27T06:17:12.325708Z","iopub.status.idle":"2025-06-27T06:17:12.330536Z","shell.execute_reply.started":"2025-06-27T06:17:12.325687Z","shell.execute_reply":"2025-06-27T06:17:12.329818Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"<h2>Check Model</h2>","metadata":{}},{"cell_type":"code","source":"model = Network(EMBEDDING_DIM).to(device)\ndummy_input = torch.randn(32, 3, INPUT_SIZE, INPUT_SIZE).to(device)\noutput = model(dummy_input)\nprint(f\"Model output shape: {output.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:17:20.782187Z","iopub.execute_input":"2025-06-27T06:17:20.782765Z","iopub.status.idle":"2025-06-27T06:17:21.067991Z","shell.execute_reply.started":"2025-06-27T06:17:20.782740Z","shell.execute_reply":"2025-06-27T06:17:21.067193Z"}},"outputs":[{"name":"stdout","text":"Model output shape: torch.Size([32, 256])\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"<h1>Test Function</h1>","metadata":{}},{"cell_type":"code","source":"def evaluate_one_shot(test_dir, model, transform, device, key=KEY_IMAGE):\n    model.eval()\n    reference_embeddings = {}\n    test_results = []\n    \n    # Process all classes\n    for class_name in sorted(os.listdir(test_dir)):\n        class_path = os.path.join(test_dir, class_name)\n        if not os.path.isdir(class_path):\n            continue\n            \n        # Find reference image\n        ref_image = None\n        for fname in os.listdir(class_path):\n            if key in fname:\n                ref_image = os.path.join(class_path, fname)\n                break\n                \n        if ref_image is None:\n            print(f\"Warning: No reference image found in {class_name}\")\n            continue\n            \n        # Load and process reference image\n        try:\n            img = cv2.imread(ref_image)\n            if img is None:\n                img = np.array(Image.open(ref_image).convert('RGB'))\n            else:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        except Exception as e:\n            print(f\"Error loading reference image {ref_image}: {e}\")\n            continue\n            \n        with torch.no_grad():\n            img_tensor = transform(image=img)[\"image\"].unsqueeze(0).to(device)\n            embedding = model(img_tensor).cpu().numpy().squeeze()\n            reference_embeddings[class_name] = embedding\n    \n    # Process test images\n    correct = 0\n    total = 0\n    \n    for class_name in sorted(os.listdir(test_dir)):\n        class_path = os.path.join(test_dir, class_name)\n        if not os.path.isdir(class_path):\n            continue\n            \n        for fname in os.listdir(class_path):\n            if key in fname:  # Skip reference images\n                continue\n                \n            img_path = os.path.join(class_path, fname)\n            try:\n                img = cv2.imread(img_path)\n                if img is None:\n                    img = np.array(Image.open(img_path).convert('RGB'))\n                else:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            except Exception as e:\n                print(f\"Error loading test image {img_path}: {e}\")\n                continue\n                \n            with torch.no_grad():\n                img_tensor = transform(image=img)[\"image\"].unsqueeze(0).to(device)\n                test_embedding = model(img_tensor).cpu().numpy().squeeze()\n            \n            # Calculate similarities\n            similarities = {}\n            for ref_class, ref_emb in reference_embeddings.items():\n                sim = cosine_similarity([test_embedding], [ref_emb])[0][0]\n                similarities[ref_class] = sim\n            \n            # Get prediction\n            pred_class = max(similarities, key=similarities.get)\n            is_correct = (pred_class == class_name)\n            \n            test_results.append({\n                'image_path': img_path,\n                'true_class': class_name,\n                'pred_class': pred_class,\n                'similarity': similarities[pred_class],\n                'is_correct': is_correct\n            })\n            \n            if is_correct:\n                correct += 1\n            total += 1\n    \n    accuracy = correct / total if total > 0 else 0.0\n    print(f\"Test Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy, test_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:17:35.688125Z","iopub.execute_input":"2025-06-27T06:17:35.688900Z","iopub.status.idle":"2025-06-27T06:17:35.698891Z","shell.execute_reply.started":"2025-06-27T06:17:35.688873Z","shell.execute_reply":"2025-06-27T06:17:35.698178Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"<h1>Initialize Dataset</h1>","metadata":{}},{"cell_type":"code","source":"# # 1. Khởi tạo datasets\n# raw_ds = OptimizedImageDataset(train_dir)\n# pair_ds = FastPairDataset(raw_ds, train_transform, max_pairs=100000)\n\n# # 2. Tạo DataLoader với num_workers cao\n# train_loader = DataLoader(\n#     pair_ds,\n#     batch_size=BATCH_SIZE,\n#     shuffle=True,\n#     num_workers=4,\n#     pin_memory=True,\n#     persistent_workers=True\n# )\n\n# # Kiểm tra một batch\n# batch = next(iter(train_loader))\n# imgs1, imgs2, labels = batch\n# print(f\"Batch shapes: imgs1 {imgs1.shape}, imgs2 {imgs2.shape}, labels {labels.shape}\")\n\n# # Hiển thị batch\n# plt.figure(figsize=(15, 7))\n# for i in range(4):\n#     plt.subplot(2, 4, i*2+1)\n#     plt.imshow(unnormalize(imgs1[i]))\n#     plt.title(f\"Label: {labels[i].item()}\")\n#     plt.axis('off')\n    \n#     plt.subplot(2, 4, i*2+2)\n#     plt.imshow(unnormalize(imgs2[i]))\n#     plt.axis('off')\n# plt.suptitle(\"Batch Samples (Left: Anchor, Right: Positive/Negative)\")\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:17:42.482239Z","iopub.execute_input":"2025-06-27T06:17:42.482785Z","iopub.status.idle":"2025-06-27T06:17:42.486471Z","shell.execute_reply.started":"2025-06-27T06:17:42.482763Z","shell.execute_reply":"2025-06-27T06:17:42.485877Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"<h1>Train Loop</h1>","metadata":{}},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer, scheduler, scaler, num_epochs, test_dir):\n    best_acc = 0.0\n    history = {'train_loss': [], 'val_acc': []}\n    \n    for epoch in range(1, num_epochs + 1):\n        start_time = time.time()\n        model.train()\n        epoch_loss = 0.0\n        \n        # Training phase\n        for batch_idx, (images, labels, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Generate pairs within batch\n            pair_indices = torch.randperm(len(labels))\n            pair_images = images[pair_indices]\n            pair_labels = labels[pair_indices]\n            \n            # Create labels for pairs (0 = same class, 1 = different class)\n            pair_targets = (labels != pair_labels).float()\n            \n            optimizer.zero_grad()\n            \n            with autocast():\n                embeddings1 = model(images)\n                embeddings2 = model(pair_images)\n                loss = criterion(embeddings1, embeddings2, pair_targets) / ACCUM_STEPS\n            \n            scaler.scale(loss).backward()\n            \n            # Gradient accumulation\n            if (batch_idx + 1) % ACCUM_STEPS == 0 or (batch_idx + 1) == len(train_loader):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n            \n            epoch_loss += loss.item() * ACCUM_STEPS\n        \n        avg_loss = epoch_loss / len(train_loader)\n        history['train_loss'].append(avg_loss)\n        \n        # Evaluation\n        if epoch % 1 == 0 or epoch == num_epochs:\n            val_acc, _ = evaluate_one_shot(test_dir, model, test_transform, device)\n            history['val_acc'].append(val_acc)\n            \n            # Save best model\n            if val_acc > best_acc:\n                best_acc = val_acc\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': avg_loss,\n                    'acc': val_acc\n                }, 'best_model.pth')\n        else:\n            val_acc = -1  # Mark as not evaluated\n        \n        # Update learning rate\n        scheduler.step(val_acc if val_acc != -1 else avg_loss)\n        \n        # Logging\n        epoch_time = time.time() - start_time\n        lr = optimizer.param_groups[0]['lr']\n        acc_str = f\"{val_acc:.4f}\" if val_acc != -1 else \"N/A\"\n        print(\n            f\"Epoch {epoch}/{num_epochs} | \"\n            f\"Loss: {avg_loss:.4f} | \"\n            f\"Acc: {acc_str} | \"\n            f\"LR: {lr:.2e} | \"\n            f\"Time: {epoch_time:.1f}s | \"\n            f\"Best Acc: {best_acc:.4f}\"\n        )\n    \n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:12:41.620414Z","iopub.execute_input":"2025-06-27T07:12:41.620730Z","iopub.status.idle":"2025-06-27T07:12:41.630498Z","shell.execute_reply.started":"2025-06-27T07:12:41.620707Z","shell.execute_reply":"2025-06-27T07:12:41.629794Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"<h1>Main Execution</h1>","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torchvision import models\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import accuracy_score\nimport warnings\nfrom collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:12:43.460589Z","iopub.execute_input":"2025-06-27T07:12:43.460861Z","iopub.status.idle":"2025-06-27T07:12:43.466431Z","shell.execute_reply.started":"2025-06-27T07:12:43.460842Z","shell.execute_reply":"2025-06-27T07:12:43.465465Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Initialize dataset and loader\n    train_dataset = ImageDataset(train_dir, transform=train_transform)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    # Initialize model, loss, optimizer\n    model = Network(emb_dim=EMBEDDING_DIM).to(device)\n    criterion = ContrastiveLoss(margin=MARGIN)\n    optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scheduler = lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n    )\n    scaler = GradScaler()\n    \n    # Start training\n    history = train(\n        model=model,\n        train_loader=train_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        scaler=scaler,\n        num_epochs=NUM_EPOCHS,\n        test_dir=test_dir\n    )\n    \n    # Final evaluation\n    print(\"\\n=== Final Evaluation ===\")\n    checkpoint = torch.load('best_model.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    final_acc, test_results = evaluate_one_shot(test_dir, model, test_transform, device)\n    print(f\"Final Accuracy: {final_acc:.4f}\")\n    \n    # Save test results\n    with open('test_results.csv', 'w') as f:\n        f.write(\"image_path,true_class,pred_class,similarity,is_correct\\n\")\n        for res in test_results:\n            f.write(f\"{res['image_path']},{res['true_class']},{res['pred_class']},{res['similarity']},{res['is_correct']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T07:12:45.211912Z","iopub.execute_input":"2025-06-27T07:12:45.212603Z"}},"outputs":[{"name":"stdout","text":"Loaded 103302 images from 135 classes\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_44/2390094001.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1/50:   0%|          | 0/1614 [00:00<?, ?it/s]/tmp/ipykernel_44/429668797.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1/50: 100%|██████████| 1614/1614 [04:39<00:00,  5.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.0916 (58/633)\nEpoch 1/50 | Loss: 0.5835 | Acc: 0.0916 | LR: 3.00e-04 | Time: 290.9s | Best Acc: 0.0916\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50:   0%|          | 0/1614 [00:00<?, ?it/s]/tmp/ipykernel_44/429668797.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 2/50: 100%|██████████| 1614/1614 [04:37<00:00,  5.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.1374 (87/633)\nEpoch 2/50 | Loss: 0.2620 | Acc: 0.1374 | LR: 3.00e-04 | Time: 288.0s | Best Acc: 0.1374\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50:   0%|          | 0/1614 [00:00<?, ?it/s]/tmp/ipykernel_44/429668797.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 3/50: 100%|██████████| 1614/1614 [04:37<00:00,  5.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.1122 (71/633)\nEpoch 3/50 | Loss: 0.3063 | Acc: 0.1122 | LR: 3.00e-04 | Time: 288.0s | Best Acc: 0.1374\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50:   0%|          | 0/1614 [00:00<?, ?it/s]/tmp/ipykernel_44/429668797.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 4/50: 100%|██████████| 1614/1614 [04:38<00:00,  5.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.1485 (94/633)\nEpoch 4/50 | Loss: 0.0386 | Acc: 0.1485 | LR: 3.00e-04 | Time: 289.0s | Best Acc: 0.1485\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50:   0%|          | 0/1614 [00:00<?, ?it/s]/tmp/ipykernel_44/429668797.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 5/50: 100%|██████████| 1614/1614 [04:38<00:00,  5.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.1422 (90/633)\nEpoch 5/50 | Loss: 0.0369 | Acc: 0.1422 | LR: 3.00e-04 | Time: 288.2s | Best Acc: 0.1485\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50:   0%|          | 0/1614 [00:00<?, ?it/s]/tmp/ipykernel_44/429668797.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 6/50: 100%|██████████| 1614/1614 [04:38<00:00,  5.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.0995 (63/633)\nEpoch 6/50 | Loss: 0.1472 | Acc: 0.0995 | LR: 3.00e-04 | Time: 288.4s | Best Acc: 0.1485\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50:   0%|          | 0/1614 [00:00<?, ?it/s]/tmp/ipykernel_44/429668797.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 7/50: 100%|██████████| 1614/1614 [04:37<00:00,  5.82it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}