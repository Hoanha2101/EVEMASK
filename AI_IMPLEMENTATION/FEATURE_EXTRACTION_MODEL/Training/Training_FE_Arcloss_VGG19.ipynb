{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9642830,"sourceType":"datasetVersion","datasetId":5888595},{"sourceId":12252246,"sourceType":"datasetVersion","datasetId":7719970}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-metric-learning -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:18:19.640295Z","iopub.execute_input":"2025-07-03T15:18:19.640461Z","iopub.status.idle":"2025-07-03T15:19:53.643393Z","shell.execute_reply.started":"2025-07-03T15:18:19.640445Z","shell.execute_reply":"2025-07-03T15:19:53.642632Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch import optim\nfrom tqdm import tqdm\nfrom torchvision import models\nimport random\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.cuda import amp\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom pytorch_metric_learning import losses\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:27:00.732728Z","iopub.execute_input":"2025-07-03T15:27:00.733391Z","iopub.status.idle":"2025-07-03T15:27:11.000111Z","shell.execute_reply.started":"2025-07-03T15:27:00.733340Z","shell.execute_reply":"2025-07-03T15:27:10.999491Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:27:11.001194Z","iopub.execute_input":"2025-07-03T15:27:11.001620Z","iopub.status.idle":"2025-07-03T15:27:11.099049Z","shell.execute_reply.started":"2025-07-03T15:27:11.001599Z","shell.execute_reply":"2025-07-03T15:27:11.098366Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"INPUT_SIZE = (224, 224)\ndata_transform = A.Compose([A.Resize(INPUT_SIZE[0], INPUT_SIZE[1])])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:42:15.600493Z","iopub.execute_input":"2025-07-03T15:42:15.601022Z","iopub.status.idle":"2025-07-03T15:42:15.613665Z","shell.execute_reply.started":"2025-07-03T15:42:15.601000Z","shell.execute_reply":"2025-07-03T15:42:15.612957Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class DATA(Dataset):\n    def __init__(self, path, transform=None, phase=\"train\"):\n        self.path = path\n        self.phase = phase\n        self.transform = transform\n        \n        folders = os.listdir(path)\n        self.image_paths = []  # ❌ Chỉ lưu đường dẫn, không load ảnh\n        self.labels = []\n        \n        self.label_dict = {}\n        for i, value in enumerate(folders):\n            self.label_dict[value] = i\n        print(self.label_dict)\n        \n        for image_folder in folders:\n            items_path = os.path.join(self.path, image_folder)\n            items_list = os.listdir(items_path)\n            \n            for image_name in items_list:\n                image_path = os.path.join(items_path, image_name)\n                self.image_paths.append(image_path)  # ✅ Chỉ lưu path\n                self.labels.append(self.label_dict[image_folder])\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # ✅ Load ảnh khi cần thiết\n        image_path = self.image_paths[idx]\n        image = cv2.imread(image_path)\n        if image is not None:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        label = self.labels[idx]\n        \n        if self.phase == \"train\":\n            if self.transform:\n                image = self.transform(image=image)[\"image\"]\n            return image, torch.tensor(label, dtype=torch.float32)\n        else:\n            if self.transform:\n                image = self.transform(image=image)[\"image\"]\n            return image, torch.tensor(label, dtype=int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:42:17.596737Z","iopub.execute_input":"2025-07-03T15:42:17.597238Z","iopub.status.idle":"2025-07-03T15:42:17.604413Z","shell.execute_reply.started":"2025-07-03T15:42:17.597195Z","shell.execute_reply":"2025-07-03T15:42:17.603824Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import time\nstart = time.time()\ntrain_path = \"/kaggle/input/data-augmented-model-2/augmented_data_model_2\"\ntrain_data =  DATA(train_path, data_transform, phase = \"train\")\nend = time.time()\nprint(f\"Load time: {round(end - start, 4)} s\")\nlen(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:42:23.852926Z","iopub.execute_input":"2025-07-03T15:42:23.853392Z","iopub.status.idle":"2025-07-03T15:42:43.255817Z","shell.execute_reply.started":"2025-07-03T15:42:23.853366Z","shell.execute_reply":"2025-07-03T15:42:43.255014Z"}},"outputs":[{"name":"stdout","text":"{'bet365': 0, 'Hermes': 1, 'danone': 2, 'netflix': 3, 'allianz': 4, 'CNN': 5, 'Prudential': 6, 'prada': 7, 'HSBC_fix35': 8, 'Meiji': 9, 'Aljazeera': 10, 'Asics': 11, 'pointsbet': 12, 'Reuters': 13, 'metlife': 14, 'National_Geographic': 15, 'Shell': 16, 'cartier': 17, 'Sheraton': 18, 'J_J': 19, 'budweiser': 20, 'aegon': 21, 'TikTok': 22, 'Dropbox': 23, 'H_M': 24, 'Deutsche_Bank': 25, 'firefox': 26, 'Volkswagen': 27, 'Panasonic': 28, 'maybach': 29, 'samsung': 30, '3M': 31, 'nvidia': 32, 'Gucci': 33, 'Tripadvisor': 34, 'Snapchat': 35, 'Xiaomi': 36, 'nivea': 37, 'Booking': 38, 'Generali': 39, 'paypal': 40, 'Ritz_Carlton': 41, 'mentos': 42, 'ebay': 43, 'gillette': 44, 'oacle': 45, 'dove': 46, 'Airbnb': 47, 'toyota': 48, 'BK8': 49, 'facebook': 50, 'Suzuki': 51, 'domino_pizza': 52, 'alpenliebe': 53, 'Fanduel': 54, 'Dell': 55, 'pizza_hut': 56, 'Marriott': 57, 'rolex': 58, 'Lenovo': 59, 'golia': 60, 'Cloudflare': 61, 'Maserati': 62, '7up': 63, 'BBC': 64, 'dafabet': 65, 'tacobell': 66, 'Citroen': 67, 'Hollywoodbet': 68, 'mastercard': 69, 'Unibet': 70, 'Citi': 71, 'avon': 72, 'lamborghini': 73, 'milo': 74, 'espn': 75, 'jollibee': 76, 'Bosch': 77, 'sony': 78, 'Jeep': 79, 'Bloomberg': 80, 'Line': 81, 'volvo': 82, 'ibm': 83, 'bally': 84, 'loreal': 85, 'Uniqlo': 86, 'baskin_robbins': 87, 'Forbes': 88, 'Colgate': 89, 'betway': 90, 'popeyes': 91, 'Cadillac': 92, 'P_G': 93, 'Louis_Vuitton': 94, 'AFC': 95, 'JPMorgan_Chase': 96, 'axa': 97, 'Mercedes_Benz': 98, 'Tumblr': 99, 'Chevrolet': 100, 'cocacola': 101, 'betmgm': 102, 'acb': 103, 'subway': 104, 'tencent': 105, 'visa': 106, 'adobe': 107, 'VK': 108, 'Ferrari': 109, 'dunkin': 110, 'Razer': 111, 'heineken': 112, 'Zara': 113, 'WeChat_FIX': 114, 'Reddit': 115, 'Audi': 116, 'tissot': 117, 'mcdonald': 118, 'tagheuer': 119, 'costco': 120, 'draftkings': 121, 'snickers': 122, 'solite': 123, 'Expedia': 124, 'Discovery': 125, 'Acura': 126, 'Subaru': 127, 'American_Express': 128, 'Hilton': 129, 'Dior': 130, 'instagram': 131, 'Bugatti': 132, 'betrivers': 133, 'corona': 134}\nLoad time: 19.3963 s\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"103302"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"anchor_img, label = train_data[100] # image at index = 100\nimg_np = anchor_img.numpy()\nimg_np = np.transpose(img_np, (1,2,0))\n\nplt.imshow(img_np)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size =  32 # Batch = 32 là max khi train với colab và kaggle, nếu lớn hơn thì out of memory -- Vram có 16gb thôi\ntrain_loader = DataLoader(train_data,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers = os.cpu_count()\n                          )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:42:52.864757Z","iopub.execute_input":"2025-07-03T15:42:52.865700Z","iopub.status.idle":"2025-07-03T15:42:52.870499Z","shell.execute_reply.started":"2025-07-03T15:42:52.865660Z","shell.execute_reply":"2025-07-03T15:42:52.869725Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=64.0, m=0.5):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        self.s = s\n        self.m = m\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.clamp(cosine ** 2, 0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1.0)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:42:56.196318Z","iopub.execute_input":"2025-07-03T15:42:56.196799Z","iopub.status.idle":"2025-07-03T15:42:56.203057Z","shell.execute_reply.started":"2025-07-03T15:42:56.196774Z","shell.execute_reply":"2025-07-03T15:42:56.202354Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha  # optional class weights\n        self.reduction = reduction\n\n    def forward(self, logits, targets):\n        targets = targets.long()\n        ce_loss = F.cross_entropy(logits, targets, reduction='none', weight=self.alpha)\n        pt = torch.exp(-ce_loss)  # pt = softmax(logits)[target]\n        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        return focal_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:43:03.287779Z","iopub.execute_input":"2025-07-03T15:43:03.288465Z","iopub.status.idle":"2025-07-03T15:43:03.294424Z","shell.execute_reply.started":"2025-07-03T15:43:03.288440Z","shell.execute_reply":"2025-07-03T15:43:03.293728Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# VGG19","metadata":{}},{"cell_type":"code","source":"model_ = models.vgg19(pretrained=True)\nclass Network(nn.Module):\n    def __init__(self, emb_dim=128):\n        super(Network, self).__init__()\n        self.conv = model_.features\n\n        self.fc = nn.Sequential(\n            nn.Linear(512*7*7, 512),\n            nn.PReLU(),\n            nn.Linear(512, emb_dim)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        # print(x.shape)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:43:06.388651Z","iopub.execute_input":"2025-07-03T15:43:06.389195Z","iopub.status.idle":"2025-07-03T15:43:11.163372Z","shell.execute_reply.started":"2025-07-03T15:43:06.389170Z","shell.execute_reply":"2025-07-03T15:43:11.162778Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 192MB/s] \n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model = Network(224).to(device)\nx = torch.rand([32, 3, 224, 224]).to(device) # input random\noutput = model(x)\nprint(output.shape) # output is torch.Size([32, 224]) -> good","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:44:29.787914Z","iopub.execute_input":"2025-07-03T15:44:29.788541Z","iopub.status.idle":"2025-07-03T15:44:29.964881Z","shell.execute_reply.started":"2025-07-03T15:44:29.788513Z","shell.execute_reply":"2025-07-03T15:44:29.964038Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 224])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"num_classes = 135\nembedding_dims = 224 #\nmodel = Network(embedding_dims).to(device)\n# criterion = SupervisedContrastiveLoss(temperature=0.1).to(device) \narc_margin = ArcMarginProduct(in_features=224, out_features=num_classes).to(device)\nfocal_loss = FocalLoss(gamma=2.0)\noptimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:44:32.172145Z","iopub.execute_input":"2025-07-03T15:44:32.172964Z","iopub.status.idle":"2025-07-03T15:44:32.302170Z","shell.execute_reply.started":"2025-07-03T15:44:32.172941Z","shell.execute_reply":"2025-07-03T15:44:32.301397Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\ndef TEST(folder_path, model, transforms, key):\n\n    label_org = []\n    dir_org = []\n    label_test = []\n    dir_test_path = []\n    dir_org_path = []\n    REFER_DICT = {}\n\n    # Put the model in evaluation mode\n    model.eval()\n\n    # Disable gradient calculation\n    with torch.no_grad():\n        # Iterate over each subfolder in the folder_path\n        for label_index, subfolder_name in enumerate(os.listdir(folder_path)):\n            REFER_DICT[label_index] = subfolder_name\n            subfolder_path = os.path.join(folder_path, subfolder_name)\n            image_files = os.listdir(subfolder_path)\n            for image_index, image_file in enumerate(image_files):\n\n                image_path = os.path.join(subfolder_path, image_file)\n\n                if key in image_path:\n\n                    image = Image.open(image_path).convert('RGB')\n                    image = transforms(image=np.array(image))[\"image\"]\n                    # Extract the embedding for the first image in the folder\n                    embedding = model(image.unsqueeze(0).to(\"cuda\"))\n                    dir_org.append(embedding)\n                    label_org.append(label_index)\n                    dir_org_path.append(image_path)\n                else:\n                    # Store the path and label for other images\n                    dir_test_path.append(image_path)\n                    label_test.append(label_index)\n\n        predict_label = []\n        Max_sim = []\n\n        # Iterate over test images\n        for test_image_path in dir_test_path:\n\n            test_image = Image.open(test_image_path).convert('RGB')\n            test_image = transforms(image=np.array(test_image))[\"image\"]\n\n            # Extract the embedding for the test image\n            test_embedding = model(test_image.unsqueeze(0).to(\"cuda\"))\n            similarities = []\n\n            # Calculate cosine similarity with each original embedding\n            for org_embedding in dir_org:\n\n                cosine_sim = cosine_similarity(org_embedding.cpu().detach().numpy(), test_embedding.cpu().detach().numpy())\n                similarities.append(cosine_sim[0][0])\n            Max_sim.append(max(similarities))\n            max_similarity_index = np.argmax(similarities)\n\n            predict_label.append(label_org[max_similarity_index])\n\n        accuracy = accuracy_score(predict_label, label_test)\n\n\n        \n        print(f'----Accuracy: {accuracy:.4f}')\n        print()\n\n        return accuracy\n\npreprocess = A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:44:35.203658Z","iopub.execute_input":"2025-07-03T15:44:35.204135Z","iopub.status.idle":"2025-07-03T15:44:35.215547Z","shell.execute_reply.started":"2025-07-03T15:44:35.204110Z","shell.execute_reply":"2025-07-03T15:44:35.214938Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import csv\nimport os\n\nepochs = 100\nmodel.train()\nscaler = torch.amp.GradScaler()\n\nACC = 0\nlog_file = \"train_log.csv\"\nmodel_save_dir = \"saved_models\"\nos.makedirs(model_save_dir, exist_ok=True)\n\n# Khởi tạo file CSV nếu chưa có\nif not os.path.exists(log_file):\n    with open(log_file, mode='w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Epoch', 'Loss', 'Accuracy'])\n\nfor epoch in tqdm(range(epochs), desc=\"Epochs\"):\n    running_loss = []\n    for step, (anchor_img, label) in enumerate(train_loader):\n        anchor_img = anchor_img.to(device).float()\n        label = label.to(device).long()  # đảm bảo đúng dtype\n\n        with torch.amp.autocast('cuda', enabled=True):\n            outputs = model(anchor_img)\n            logits = arc_margin(outputs, label)\n            loss = focal_loss(logits, label)\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % 4 == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        running_loss.append(loss.cpu().detach().numpy())\n\n    avg_loss = np.mean(running_loss)\n\n    # Tính accuracy mỗi 10 epoch\n    if (epoch + 1) % 10 == 0:\n        test_path = \"/kaggle/input/logo-verify-test/logo_verify_test\"  # đường dẫn test\n        accuracy = TEST(test_path, model, preprocess, key=\"000000\")\n        model.train()  # quay lại train mode\n\n        # Lưu model nếu tốt nhất\n        if accuracy >= ACC:\n            best_path = f\"/kaggle/working/model_best{accuracy*1000:.0f}.pth\"\n            torch.save(model, best_path)\n            ACC = accuracy\n    else:\n        accuracy = None  # không test\n\n    # Ghi log CSV\n    with open(log_file, mode='a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([epoch + 1, avg_loss, accuracy if accuracy is not None else \"\"])\n\n    # Lưu model mỗi 5 epoch\n    if (epoch + 1) % 10 == 0:\n        torch.save(model, f\"{model_save_dir}/model_epoch_{epoch + 1}.pth\")\n\n    print(f\"Epoch: {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}