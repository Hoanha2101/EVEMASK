{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-metric-learning -q\n",
    "!pip install matplotlib -q\n",
    "!pip install tqdm -q\n",
    "!pip install albumentations -q\n",
    "!pip install timm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda import amp\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444debb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe875ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (224, 224)\n",
    "data_transform = A.Compose([A.Resize(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.VerticalFlip(p=0.5),\n",
    "                        A.Rotate(limit=45, p=1.0),\n",
    "                        A.CoarseDropout(\n",
    "                                    max_holes=8,                \n",
    "                                    max_height=16,              \n",
    "                                    max_width=16,               \n",
    "                                    min_holes=1,                \n",
    "                                    min_height=8,               \n",
    "                                    min_width=8,                \n",
    "                                    fill_value=0,               \n",
    "                                    p=0.5                        \n",
    "                                ),\n",
    "                        A.RandomBrightnessContrast(\n",
    "                                brightness_limit=(-0.1,0.1), \n",
    "                                contrast_limit=(-0.1, 0.1), \n",
    "                                p=0.5),\n",
    "                        A.Normalize(\n",
    "                                mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225], \n",
    "                                max_pixel_value=255.0, \n",
    "                                p=1.0),\n",
    "                        ToTensorV2()], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea20fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi√™n b·∫£n ƒë∆°n gi·∫£n ch·ªâ v·ªõi Resize + Normalize\n",
    "data_transform_simple = A.Compose([\n",
    "    A.Resize(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225], \n",
    "        max_pixel_value=255.0, \n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], p=1.)\n",
    "\n",
    "# Phi√™n b·∫£n trung b√¨nh v·ªõi m·ªôt s·ªë augmentation c∆° b·∫£n\n",
    "data_transform_moderate = A.Compose([\n",
    "    A.Resize(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=(-0.1,0.1), \n",
    "        contrast_limit=(-0.1, 0.1), \n",
    "        p=0.3\n",
    "    ),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225], \n",
    "        max_pixel_value=255.0, \n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], p=1.)\n",
    "\n",
    "print(\"üìä So s√°nh c√°c phi√™n b·∫£n data augmentation:\")\n",
    "print(\"1. data_transform (hi·ªán t·∫°i): Nhi·ªÅu augmentation - t·ªët cho generalization nh∆∞ng c√≥ th·ªÉ l√†m ch·∫≠m training\")\n",
    "print(\"2. data_transform_simple: Ch·ªâ resize + normalize - training nhanh h∆°n, c√≥ th·ªÉ overfit\")\n",
    "print(\"3. data_transform_moderate: Trung b√¨nh - balance gi·ªØa speed v√† generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b793791",
   "metadata": {},
   "source": [
    "## üîç Ph√¢n t√≠ch: T√°c ƒë·ªông c·ªßa vi·ªác ch·ªâ s·ª≠ d·ª•ng Resize\n",
    "\n",
    "### ‚úÖ **∆Øu ƒëi·ªÉm khi ch·ªâ d√πng Resize + Normalize:**\n",
    "\n",
    "1. **Training nhanh h∆°n**: √çt ph√©p bi·∫øn ƒë·ªïi ‚Üí t·ªëc ƒë·ªô training tƒÉng ƒë√°ng k·ªÉ\n",
    "2. **·ªîn ƒë·ªãnh h∆°n**: √çt nhi·ªÖu trong qu√° tr√¨nh training ‚Üí loss curve m∆∞·ª£t h∆°n\n",
    "3. **Ph√π h·ª£p v·ªõi d·ªØ li·ªáu ƒë√£ augmented**: N·∫øu dataset ƒë√£ ƒë∆∞·ª£c augment tr∆∞·ªõc th√¨ kh√¥ng c·∫ßn th√™m augmentation\n",
    "4. **Memory usage th·∫•p h∆°n**: √çt operations ‚Üí √≠t GPU memory\n",
    "\n",
    "### ‚ö†Ô∏è **Nh∆∞·ª£c ƒëi·ªÉm:**\n",
    "\n",
    "1. **D·ªÖ overfit**: Model c√≥ th·ªÉ h·ªçc thu·ªôc l√≤ng training data\n",
    "2. **Generalization k√©m**: K√©m kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu m·ªõi v·ªõi conditions kh√°c\n",
    "3. **Robustness th·∫•p**: K√©m kh·∫£ nƒÉng ch·ªëng nhi·ªÖu, thay ƒë·ªïi √°nh s√°ng, g√≥c quay\n",
    "\n",
    "### üéØ **Khuy·∫øn ngh·ªã:**\n",
    "\n",
    "- **N·∫øu dataset l·ªõn (>10k images/class)**: C√≥ th·ªÉ d√πng simple transform\n",
    "- **N·∫øu dataset nh·ªè (<5k images/class)**: N√™n gi·ªØ moderate augmentation\n",
    "- **Cho Contrastive Learning**: Moderate augmentation th∆∞·ªùng t·ªët h∆°n v√¨ gi√∫p model h·ªçc ƒë∆∞·ª£c invariant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test ƒë·ªÉ ch·ªçn transform ph√π h·ª£p\n",
    "import time\n",
    "\n",
    "def test_transform_speed(transform, num_samples=100):\n",
    "    \"\"\"Test t·ªëc ƒë·ªô c·ªßa transform\"\"\"\n",
    "    # T·∫°o fake image\n",
    "    fake_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(num_samples):\n",
    "        transformed = transform(image=fake_image)[\"image\"]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_samples * 1000  # ms\n",
    "    return avg_time\n",
    "\n",
    "print(\"‚è±Ô∏è So s√°nh t·ªëc ƒë·ªô transform (ms/image):\")\n",
    "print(f\"Original (nhi·ªÅu augment): {test_transform_speed(data_transform):.2f} ms\")\n",
    "print(f\"Simple (ch·ªâ resize): {test_transform_speed(data_transform_simple):.2f} ms\") \n",
    "print(f\"Moderate (v·ª´a ph·∫£i): {test_transform_speed(data_transform_moderate):.2f} ms\")\n",
    "\n",
    "# ƒê·ªÉ test th·ª±c t·∫ø, b·∫°n c√≥ th·ªÉ thay ƒë·ªïi transform trong DATA class:\n",
    "print(\"\\nüí° ƒê·ªÉ test:\")\n",
    "print(\"1. Thay 'data_transform' b·∫±ng 'data_transform_simple' trong DATA class\")\n",
    "print(\"2. Train m·ªôt v√†i epoch v√† so s√°nh k·∫øt qu·∫£\")\n",
    "print(\"3. Quan s√°t validation accuracy v√† training speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(mode=\"original\"):\n",
    "    \"\"\"\n",
    "    Utility function ƒë·ªÉ ch·ªçn transform\n",
    "    mode: \"original\", \"simple\", \"moderate\"\n",
    "    \"\"\"\n",
    "    INPUT_SIZE = (224, 224)\n",
    "    \n",
    "    if mode == \"simple\":\n",
    "        return A.Compose([\n",
    "            A.Resize(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2()\n",
    "        ], p=1.)\n",
    "    \n",
    "    elif mode == \"moderate\":\n",
    "        return A.Compose([\n",
    "            A.Resize(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2()\n",
    "        ], p=1.)\n",
    "    \n",
    "    else:  # original\n",
    "        return A.Compose([\n",
    "            A.Resize(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Rotate(limit=45, p=1.0),\n",
    "            A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, min_height=8, min_width=8, fill_value=0, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2()\n",
    "        ], p=1.)\n",
    "\n",
    "# Example usage:\n",
    "# train_data = DATA(train_path, get_transform(\"simple\"), phase=\"train\")\n",
    "\n",
    "print(\"üîß S·ª≠ d·ª•ng: get_transform('simple'), get_transform('moderate'), ho·∫∑c get_transform('original')\")\n",
    "print(\"\\nüìà Th·ªëng k√™ d·ª± ƒëo√°n:\")\n",
    "print(\"‚Ä¢ Simple: Training nhanh nh·∫•t, c√≥ th·ªÉ overfit n·∫øu dataset nh·ªè\")\n",
    "print(\"‚Ä¢ Moderate: Balance t·ªët gi·ªØa speed v√† performance\") \n",
    "print(\"‚Ä¢ Original: Generalization t·ªët nh·∫•t nh∆∞ng training ch·∫≠m h∆°n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA(Dataset):\n",
    "    def __init__(self, path, transform=None, phase=\"train\"):\n",
    "        self.path = path\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "        \n",
    "        folders = os.listdir(path)\n",
    "        self.image_paths = []  # ‚ùå Ch·ªâ l∆∞u ƒë∆∞·ªùng d·∫´n, kh√¥ng load ·∫£nh\n",
    "        self.labels = []\n",
    "        \n",
    "        self.label_dict = {}\n",
    "        for i, value in enumerate(folders):\n",
    "            self.label_dict[value] = i\n",
    "        print(self.label_dict)\n",
    "        \n",
    "        for image_folder in folders:\n",
    "            items_path = os.path.join(self.path, image_folder)\n",
    "            items_list = os.listdir(items_path)\n",
    "            \n",
    "            for image_name in items_list:\n",
    "                image_path = os.path.join(items_path, image_name)\n",
    "                self.image_paths.append(image_path)  # ‚úÖ Ch·ªâ l∆∞u path\n",
    "                self.labels.append(self.label_dict[image_folder])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ‚úÖ Load ·∫£nh khi c·∫ßn thi·∫øt\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.phase == \"train\":\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)[\"image\"]\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)[\"image\"]\n",
    "            return image, torch.tensor(label, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7879170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "train_path = \"/kaggle/input/data-augmented-model-2/augmented_data_model_2\"\n",
    "train_data =  DATA(train_path, data_transform, phase = \"train\")\n",
    "end = time.time()\n",
    "print(f\"Load time: {round(end - start, 4)} s\")\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_img, label = train_data[100] # image at index = 100\n",
    "img_np = anchor_img.numpy()\n",
    "img_np = np.transpose(img_np, (1,2,0))\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  32 # Batch = 32 l√† max khi train v·ªõi colab v√† kaggle, n·∫øu l·ªõn h∆°n th√¨ out of memory -- Vram c√≥ 16gb th√¥i\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers = os.cpu_count()\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised Contrastive Loss implementation - Improved version\n",
    "    Ref: https://arxiv.org/abs/2004.11362\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.1, base_temperature=0.07):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, feature_dim]\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Ensure labels are correct type\n",
    "        labels = labels.long().view(-1)\n",
    "        \n",
    "        if len(features.shape) < 3:\n",
    "            features = features.unsqueeze(1)\n",
    "            \n",
    "        # Normalize features\n",
    "        features = F.normalize(features, p=2, dim=2)\n",
    "        \n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        \n",
    "        anchor_feature = contrast_feature\n",
    "        anchor_count = contrast_count\n",
    "        \n",
    "        # Compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        \n",
    "        # For numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        \n",
    "        # Create label mask\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        if labels.shape[0] != batch_size:\n",
    "            raise ValueError('Num of labels does not match num of features')\n",
    "            \n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        \n",
    "        # Mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # Compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Compute mean of log-likelihood over positive\n",
    "        # Only compute loss for samples that have positive pairs\n",
    "        valid_samples = mask.sum(1) > 0\n",
    "        if valid_samples.sum() == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True).to(device)\n",
    "            \n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
    "        \n",
    "        # Loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos[valid_samples]\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a912c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "        # B·ªè layer cu·ªëi c√πng (fc)\n",
    "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # Output: [B, 2048, 1, 1]\n",
    "\n",
    "        # FC Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)           # [B, 2048, 1, 1]\n",
    "        x = torch.flatten(x, 1)        # [B, 2048]\n",
    "        x = self.fc(x)                 # [B, emb_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(256).to(device)\n",
    "x = torch.rand([32, 3, 224, 224]).to(device) # input random\n",
    "output = model(x)\n",
    "print(output.shape) # output is torch.Size([32, 256]) -> good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 256 #\n",
    "model = Network(embedding_dims).to(device)\n",
    "criterion = SupervisedContrastiveLoss(temperature=0.1).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "def TEST(folder_path, model, transforms, key):\n",
    "\n",
    "    label_org = []\n",
    "    dir_org = []\n",
    "    label_test = []\n",
    "    dir_test_path = []\n",
    "    dir_org_path = []\n",
    "    REFER_DICT = {}\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over each subfolder in the folder_path\n",
    "        for label_index, subfolder_name in enumerate(os.listdir(folder_path)):\n",
    "            REFER_DICT[label_index] = subfolder_name\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            image_files = os.listdir(subfolder_path)\n",
    "            for image_index, image_file in enumerate(image_files):\n",
    "\n",
    "                image_path = os.path.join(subfolder_path, image_file)\n",
    "\n",
    "                if key in image_path:\n",
    "\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    image = transforms(image=np.array(image))[\"image\"]\n",
    "                    # Extract the embedding for the first image in the folder\n",
    "                    embedding = model(image.unsqueeze(0).to(\"cuda\"))\n",
    "                    dir_org.append(embedding)\n",
    "                    label_org.append(label_index)\n",
    "                    dir_org_path.append(image_path)\n",
    "                else:\n",
    "                    # Store the path and label for other images\n",
    "                    dir_test_path.append(image_path)\n",
    "                    label_test.append(label_index)\n",
    "\n",
    "        predict_label = []\n",
    "        Max_sim = []\n",
    "\n",
    "        if not dir_org:  # N·∫øu kh√¥ng c√≥ ·∫£nh reference\n",
    "            print(f\"Warning: No reference images found with key '{key}'\")\n",
    "            return 0.0\n",
    "\n",
    "        # Iterate over test images\n",
    "        for test_image_path in dir_test_path:\n",
    "\n",
    "            test_image = Image.open(test_image_path).convert('RGB')\n",
    "            test_image = transforms(image=np.array(test_image))[\"image\"]\n",
    "\n",
    "            # Extract the embedding for the test image\n",
    "            test_embedding = model(test_image.unsqueeze(0).to(\"cuda\"))\n",
    "            similarities = []\n",
    "\n",
    "            # Calculate cosine similarity with each original embedding\n",
    "            for org_embedding in dir_org:\n",
    "\n",
    "                cosine_sim = cosine_similarity(org_embedding.cpu().detach().numpy(), test_embedding.cpu().detach().numpy())\n",
    "                similarities.append(cosine_sim[0][0])\n",
    "\n",
    "            if similarities:\n",
    "                Max_sim.append(max(similarities))\n",
    "                max_similarity_index = np.argmax(similarities)\n",
    "                predict_label.append(label_org[max_similarity_index])\n",
    "            else:\n",
    "                print(f\"Warning: No similarities calculated for {test_image_path}\")\n",
    "                predict_label.append(-1)  # Ho·∫∑c m·ªôt gi√° tr·ªã m·∫∑c ƒë·ªãnh\n",
    "        if not predict_label or not label_test:\n",
    "            print(\"Warning: No predictions or labels to evaluate\")\n",
    "            return 0.0\n",
    "                \n",
    "        accuracy = accuracy_score(predict_label, label_test)\n",
    "        print(f'----Key: {key}')\n",
    "        print(f'----Number of test images: {len(label_test)}')\n",
    "        print(f'----Number of reference images: {len(label_org)}')\n",
    "        print(f'----Accuracy: {accuracy:.4f}')\n",
    "        print()\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "preprocess = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# Kh·ªüi t·∫°o lists ƒë·ªÉ l∆∞u training history\n",
    "train_history = {\n",
    "    'epoch': [],\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "    'learning_rate': [],\n",
    "    'timestamp': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_results():\n",
    "    os.makedirs('/kaggle/working/outputs', exist_ok=True)\n",
    "    \n",
    "    # Save training history\n",
    "    df_history = pd.DataFrame(train_history)\n",
    "    df_history.to_csv('/kaggle/working/outputs/training_history.csv', index=False)\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = f\"/kaggle/working/outputs/final_model_epoch_{epochs}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'final_accuracy': ACC,\n",
    "        'training_history': train_history\n",
    "    }, final_model_path)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'total_epochs': epochs,\n",
    "        'best_accuracy': float(ACC),\n",
    "        'final_loss': float(train_history['loss'][-1]) if train_history['loss'] else 0.0,\n",
    "        'best_model_path': best_model_path,\n",
    "        'final_model_path': final_model_path,\n",
    "        'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_samples': len(train_data),\n",
    "        'batch_size': batch_size,\n",
    "        'embedding_dims': embedding_dims\n",
    "    }\n",
    "    \n",
    "    with open('/kaggle/working/outputs/training_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    results_df = pd.DataFrame([summary])\n",
    "    results_df.to_csv('/kaggle/working/outputs/final_results.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved! Best Accuracy: {ACC:.4f}\")\n",
    "\n",
    "# Signal handler\n",
    "def signal_handler(sig, frame):\n",
    "    print('\\nTraining interrupted! Saving current progress...')\n",
    "    save_final_results()\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45523c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "model.train()\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "ACC = 0\n",
    "best_model_path = None\n",
    "\n",
    "# ‚úÖ S·ª≠a l·ªói typo ƒë∆∞·ªùng d·∫´n\n",
    "os.makedirs('/kaggle/working/outputs', exist_ok=True)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "\n",
    "    for step, (anchor_img, label) in enumerate(train_loader):\n",
    "        anchor_img = anchor_img.to(device).float()\n",
    "        label = label.to(device).long()\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=True):\n",
    "            outputs = model(anchor_img)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss = loss / 4\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % 4 == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (step + 1) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    epoch_loss = np.mean(running_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        test_path = \"/kaggle/input/logo-verify-test/logo_verify_test\"\n",
    "        accuracy = TEST(test_path, model, preprocess, key=\"000000\")\n",
    "        model.train()\n",
    "        \n",
    "        if accuracy >= ACC:\n",
    "            if best_model_path and os.path.exists(best_model_path):\n",
    "                os.remove(best_model_path)\n",
    "            \n",
    "            # ‚úÖ ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "            os.makedirs('/kaggle/working/outputs', exist_ok=True)\n",
    "            best_model_path = f\"/kaggle/working/outputs/model_best_epoch_{epoch+1}_acc_{accuracy:.4f}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'accuracy': accuracy,\n",
    "                'loss': epoch_loss\n",
    "            }, best_model_path)\n",
    "            ACC = accuracy\n",
    "    else:\n",
    "        accuracy = None\n",
    "    \n",
    "    # L∆∞u training history\n",
    "    train_history['epoch'].append(epoch + 1)\n",
    "    train_history['loss'].append(float(epoch_loss))\n",
    "    train_history['accuracy'].append(float(accuracy) if accuracy is not None else None)\n",
    "    train_history['learning_rate'].append(float(current_lr))\n",
    "    train_history['timestamp'].append(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    print(\"Epoch: {}/{} - Loss: {:.4f} - LR: {:.2e}{}\".format(\n",
    "        epoch+1, epochs, epoch_loss, current_lr,\n",
    "        f\" - Accuracy: {accuracy:.4f}\" if accuracy is not None else \"\"\n",
    "    ))\n",
    "    \n",
    "    # ‚úÖ ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i tr∆∞·ªõc m·ªói l·∫ßn l∆∞u\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        os.makedirs('/kaggle/working/outputs', exist_ok=True)\n",
    "        df_history = pd.DataFrame(train_history)\n",
    "        df_history.to_csv('/kaggle/working/outputs/training_history.csv', index=False)\n",
    "        print(f\"Training history saved at epoch {epoch+1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*50)\n",
    "save_final_results()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
