{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca027288",
   "metadata": {},
   "source": [
    "# Logo Similarity Analysis using Cosine Similarity\n",
    "## Load trained models and test logo similarity with comprehensive visualization\n",
    "\n",
    "This notebook performs comprehensive analysis of logo similarity using trained models:\n",
    "- Loads pre-trained models with proper error handling\n",
    "- Calculates cosine similarity between logo embeddings\n",
    "- Provides detailed visualization and analysis\n",
    "- Tests model performance on logo verification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c94d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix environment setup and imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix numpy compatibility issues\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "# Import libraries step by step to avoid conflicts\n",
    "import numpy as np\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda import amp\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import sklearn with error handling\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"‚úÖ Sklearn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Sklearn import error: {e}\")\n",
    "    print(\"Installing scikit-learn...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\"])\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual cosine similarity implementation as backup\n",
    "def cosine_similarity_manual(X, Y=None):\n",
    "    \"\"\"Manual implementation of cosine similarity to avoid sklearn issues\"\"\"\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    \n",
    "    # Normalize vectors\n",
    "    X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-8)\n",
    "    Y_norm = Y / (np.linalg.norm(Y, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    return np.dot(X_norm, Y_norm.T)\n",
    "\n",
    "# Use manual implementation if sklearn fails\n",
    "try:\n",
    "    # Test sklearn cosine_similarity\n",
    "    test_a = np.random.randn(2, 3)\n",
    "    test_b = np.random.randn(2, 3)\n",
    "    _ = cosine_similarity(test_a, test_b)\n",
    "    print(\"‚úÖ Using sklearn cosine_similarity\")\n",
    "except:\n",
    "    cosine_similarity = cosine_similarity_manual\n",
    "    print(\"‚ö†Ô∏è Using manual cosine_similarity implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a218fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture - flexible for different backbones\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=256, backbone='mobilenet_v2'):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        if backbone == 'mobilenet_v2':\n",
    "            base_model = models.mobilenet_v2(pretrained=True)\n",
    "            self.backbone = base_model.features\n",
    "            in_features = 1280\n",
    "        elif backbone == 'vgg16':\n",
    "            base_model = models.vgg16(pretrained=True)\n",
    "            self.backbone = base_model.features\n",
    "            in_features = 512 * 7 * 7  # VGG16 output\n",
    "        elif backbone == 'resnet18':\n",
    "            base_model = models.resnet18(pretrained=True)\n",
    "            self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
    "            in_features = 512\n",
    "        elif backbone == 'resnet50':\n",
    "            base_model = models.resnet50(pretrained=True)\n",
    "            self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
    "            in_features = 2048\n",
    "        else:\n",
    "            # Default to MobileNetV2\n",
    "            base_model = models.mobilenet_v2(pretrained=True)\n",
    "            self.backbone = base_model.features\n",
    "            in_features = 1280\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Load trained model with comprehensive error handling\n",
    "def load_trained_model(model_paths, backbone='mobilenet_v2', emb_dim=256):\n",
    "    \"\"\"\n",
    "    Load trained model with multiple fallback options\n",
    "    \n",
    "    Args:\n",
    "        model_paths: list of potential model file paths\n",
    "        backbone: backbone architecture type\n",
    "        emb_dim: embedding dimension\n",
    "    \n",
    "    Returns:\n",
    "        loaded model and success status\n",
    "    \"\"\"\n",
    "    model = Network(emb_dim=emb_dim, backbone=backbone).to(device)\n",
    "    model_loaded = False\n",
    "    loaded_info = {}\n",
    "    \n",
    "    # Try loading from multiple paths\n",
    "    for model_path in model_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                # Method 1: Load with weights_only=False (for newer PyTorch)\n",
    "                checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "                \n",
    "                if isinstance(checkpoint, dict):\n",
    "                    if 'model_state_dict' in checkpoint:\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        loaded_info = {\n",
    "                            'path': model_path,\n",
    "                            'accuracy': checkpoint.get('accuracy', 'N/A'),\n",
    "                            'epoch': checkpoint.get('epoch', 'N/A')\n",
    "                        }\n",
    "                    elif 'state_dict' in checkpoint:\n",
    "                        model.load_state_dict(checkpoint['state_dict'])\n",
    "                        loaded_info = {'path': model_path, 'accuracy': 'N/A', 'epoch': 'N/A'}\n",
    "                    else:\n",
    "                        # Assume checkpoint is direct state_dict\n",
    "                        model.load_state_dict(checkpoint)\n",
    "                        loaded_info = {'path': model_path, 'accuracy': 'N/A', 'epoch': 'N/A'}\n",
    "                else:\n",
    "                    # Checkpoint is the model itself\n",
    "                    model = checkpoint.to(device)\n",
    "                    loaded_info = {'path': model_path, 'accuracy': 'N/A', 'epoch': 'N/A'}\n",
    "                \n",
    "                print(f\"‚úÖ Model loaded successfully from: {os.path.basename(model_path)}\")\n",
    "                print(f\"   Accuracy: {loaded_info.get('accuracy', 'N/A')}\")\n",
    "                print(f\"   Epoch: {loaded_info.get('epoch', 'N/A')}\")\n",
    "                model_loaded = True\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {os.path.basename(model_path)}: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"üìÅ Path not found: {os.path.basename(model_path)}\")\n",
    "\n",
    "    if not model_loaded:\n",
    "        print(\"‚ö†Ô∏è No model loaded successfully. Using randomly initialized model.\")\n",
    "        print(\"   Results will not be meaningful!\")\n",
    "        loaded_info = {'path': 'random_init', 'accuracy': 0.0, 'epoch': 0}\n",
    "\n",
    "    model.eval()\n",
    "    return model, model_loaded, loaded_info\n",
    "\n",
    "# Define potential model paths\n",
    "model_paths = [\n",
    "    \"SupConLoss_BBMobileNetV2.pth\",\n",
    "    \"Model2/SupConLoss_BBMobileNetV2.pth\", \n",
    "    \"SupConLoss_BBModel2/Model2/SupConLoss_BBMobileNetV2.pth\",\n",
    "    \"../SupConLoss_BBMobileNetV2.pth\",\n",
    "    \"outputs/model_best.pth\",\n",
    "    \"outputs/SupConLoss_BBMobileNetV2.pth\"\n",
    "]\n",
    "\n",
    "# Load model\n",
    "model, model_loaded, model_info = load_trained_model(model_paths, backbone='mobilenet_v2', emb_dim=256)\n",
    "print(f\"\\nModel Status: {'Loaded' if model_loaded else 'Random Init'}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8808142",
   "metadata": {},
   "outputs": [],
   "source": [
    "<VSCode.Cell language=\"python\">\n",
    "def calculate_cosine_similarity_matrix_enhanced(folder_path, model, transforms, key=\"000000\", max_samples=100):\n",
    "    \"\"\"\n",
    "    Enhanced cosine similarity calculation with better error handling and efficiency\n",
    "    \n",
    "    Args:\n",
    "        folder_path: path to test data folder\n",
    "        model: trained model\n",
    "        transforms: preprocessing transforms\n",
    "        key: key to distinguish reference vs test images\n",
    "        max_samples: limit samples to avoid memory issues\n",
    "    \n",
    "    Returns:\n",
    "        similarity_data: dict containing similarity information\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"‚ùå Folder not found: {folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    label_org = []\n",
    "    embeddings_org = []\n",
    "    paths_org = []\n",
    "    label_test = []\n",
    "    embeddings_test = []\n",
    "    paths_test = []\n",
    "    \n",
    "    REFER_DICT = {}\n",
    "    \n",
    "    print(\"üîç Scanning for images...\")\n",
    "    \n",
    "    # Collect all image paths first\n",
    "    all_ref_paths = []\n",
    "    all_test_paths = []\n",
    "    all_ref_labels = []\n",
    "    all_test_labels = []\n",
    "    \n",
    "    try:\n",
    "        subfolders = [f for f in os.listdir(folder_path) \n",
    "                     if os.path.isdir(os.path.join(folder_path, f))]\n",
    "        \n",
    "        for label_index, subfolder_name in enumerate(subfolders):\n",
    "            REFER_DICT[label_index] = subfolder_name\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            \n",
    "            if not os.path.exists(subfolder_path):\n",
    "                continue\n",
    "                \n",
    "            # Get all image files\n",
    "            image_files = [f for f in os.listdir(subfolder_path) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "            \n",
    "            ref_count = 0\n",
    "            test_count = 0\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(subfolder_path, image_file)\n",
    "                \n",
    "                if key in image_file and ref_count < max_samples//2:\n",
    "                    # Reference images\n",
    "                    all_ref_paths.append(image_path)\n",
    "                    all_ref_labels.append(label_index)\n",
    "                    ref_count += 1\n",
    "                    \n",
    "                elif key not in image_file and test_count < max_samples//2:\n",
    "                    # Test images\n",
    "                    all_test_paths.append(image_path)\n",
    "                    all_test_labels.append(label_index)\n",
    "                    test_count += 1\n",
    "        \n",
    "        print(f\"üìä Found {len(all_ref_paths)} reference images and {len(all_test_paths)} test images\")\n",
    "        \n",
    "        if not all_ref_paths or not all_test_paths:\n",
    "            print(f\"‚ö†Ô∏è No images found with key '{key}' for reference or no test images found\")\n",
    "            return None\n",
    "        \n",
    "        # Extract embeddings in batches\n",
    "        print(\"üîÆ Extracting reference embeddings...\")\n",
    "        embeddings_org, paths_org = extract_embeddings_batch(all_ref_paths, model, transforms)\n",
    "        label_org = [all_ref_labels[all_ref_paths.index(path)] for path in paths_org]\n",
    "        \n",
    "        print(\"üîÆ Extracting test embeddings...\")\n",
    "        embeddings_test, paths_test = extract_embeddings_batch(all_test_paths, model, transforms)\n",
    "        label_test = [all_test_labels[all_test_paths.index(path)] for path in paths_test]\n",
    "        \n",
    "        if not embeddings_org or not embeddings_test:\n",
    "            print(\"‚ùå No valid embeddings extracted!\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        embeddings_org = np.vstack(embeddings_org)\n",
    "        embeddings_test = np.vstack(embeddings_test)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully extracted embeddings:\")\n",
    "        print(f\"   Reference: {embeddings_org.shape}\")\n",
    "        print(f\"   Test: {embeddings_test.shape}\")\n",
    "        \n",
    "        # Calculate cosine similarity matrix\n",
    "        print(\"üìê Calculating cosine similarity matrix...\")\n",
    "        similarity_matrix = cosine_similarity(embeddings_test, embeddings_org)\n",
    "        \n",
    "        print(f\"   Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'similarity_matrix': similarity_matrix,\n",
    "            'embeddings_org': embeddings_org,\n",
    "            'embeddings_test': embeddings_test,\n",
    "            'label_org': label_org,\n",
    "            'label_test': label_test,\n",
    "            'paths_org': paths_org,\n",
    "            'paths_test': paths_test,\n",
    "            'refer_dict': REFER_DICT\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in similarity calculation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a89f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_similarity_distribution(similarity_data):\n",
    "    \"\"\"Visualize cosine similarity distribution\"\"\"\n",
    "    \n",
    "    similarity_matrix = similarity_data['similarity_matrix']\n",
    "    label_test = similarity_data['label_test']\n",
    "    label_org = similarity_data['label_org']\n",
    "    refer_dict = similarity_data['refer_dict']\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    positive_similarities = []  # Same class\n",
    "    negative_similarities = []  # Different class\n",
    "    \n",
    "    for i, test_label in enumerate(label_test):\n",
    "        for j, ref_label in enumerate(label_org):\n",
    "            sim_score = similarity_matrix[i, j]\n",
    "            if test_label == ref_label:\n",
    "                positive_similarities.append(sim_score)\n",
    "            else:\n",
    "                negative_similarities.append(sim_score)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # 1. Histogram of similarity scores\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(positive_similarities, bins=50, alpha=0.7, label='Same Class (Positive)', color='green', density=True)\n",
    "    plt.hist(negative_similarities, bins=50, alpha=0.7, label='Different Class (Negative)', color='red', density=True)\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Cosine Similarity Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot\n",
    "    plt.subplot(2, 3, 2)\n",
    "    data_for_box = [positive_similarities, negative_similarities]\n",
    "    labels_for_box = ['Same Class', 'Different Class']\n",
    "    plt.boxplot(data_for_box, labels=labels_for_box)\n",
    "    plt.ylabel('Cosine Similarity')\n",
    "    plt.title('Similarity Score Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Heatmap of similarity matrix (sample)\n",
    "    plt.subplot(2, 3, 3)\n",
    "    sample_size = min(20, similarity_matrix.shape[0], similarity_matrix.shape[1])\n",
    "    sample_matrix = similarity_matrix[:sample_size, :sample_size]\n",
    "    sns.heatmap(sample_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "    plt.title(f'Similarity Matrix Heatmap (Sample {sample_size}x{sample_size})')\n",
    "    plt.xlabel('Reference Images')\n",
    "    plt.ylabel('Test Images')\n",
    "    \n",
    "    # 4. Statistics summary\n",
    "    plt.subplot(2, 3, 4)\n",
    "    stats_data = {\n",
    "        'Metric': ['Mean', 'Std', 'Min', 'Max', 'Count'],\n",
    "        'Same Class': [\n",
    "            np.mean(positive_similarities),\n",
    "            np.std(positive_similarities),\n",
    "            np.min(positive_similarities),\n",
    "            np.max(positive_similarities),\n",
    "            len(positive_similarities)\n",
    "        ],\n",
    "        'Different Class': [\n",
    "            np.mean(negative_similarities),\n",
    "            np.std(negative_similarities),\n",
    "            np.min(negative_similarities),\n",
    "            np.max(negative_similarities),\n",
    "            len(negative_similarities)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "    table = plt.table(cellText=stats_df.values, colLabels=stats_df.columns, \n",
    "                     cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    plt.title('Similarity Statistics Summary')\n",
    "    \n",
    "    # 5. Threshold analysis\n",
    "    plt.subplot(2, 3, 5)\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    accuracies = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, test_label in enumerate(label_test):\n",
    "            max_sim = np.max(similarity_matrix[i])\n",
    "            max_idx = np.argmax(similarity_matrix[i])\n",
    "            predicted_label = label_org[max_idx]\n",
    "            \n",
    "            if max_sim >= threshold:\n",
    "                total += 1\n",
    "                if predicted_label == test_label:\n",
    "                    correct += 1\n",
    "        \n",
    "        if total > 0:\n",
    "            accuracies.append(correct / total)\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "    \n",
    "    plt.plot(thresholds, accuracies)\n",
    "    plt.xlabel('Similarity Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Similarity Threshold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Precision-Recall curve approximation\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    # Calculate optimal threshold\n",
    "    best_threshold = thresholds[np.argmax(accuracies)]\n",
    "    best_accuracy = np.max(accuracies)\n",
    "    \n",
    "    plt.axvline(x=best_threshold, color='red', linestyle='--', \n",
    "                label=f'Best Threshold: {best_threshold:.3f}\\nAccuracy: {best_accuracy:.3f}')\n",
    "    plt.plot(thresholds, accuracies, 'b-', linewidth=2)\n",
    "    plt.xlabel('Similarity Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Optimal Threshold Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COSINE SIMILARITY ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total test samples: {len(label_test)}\")\n",
    "    print(f\"Total reference samples: {len(label_org)}\")\n",
    "    print(f\"Number of classes: {len(set(label_test))}\")\n",
    "    print(f\"\\nSame Class Similarities:\")\n",
    "    print(f\"  Mean: {np.mean(positive_similarities):.4f}\")\n",
    "    print(f\"  Std:  {np.std(positive_similarities):.4f}\")\n",
    "    print(f\"  Range: [{np.min(positive_similarities):.4f}, {np.max(positive_similarities):.4f}]\")\n",
    "    print(f\"\\nDifferent Class Similarities:\")\n",
    "    print(f\"  Mean: {np.mean(negative_similarities):.4f}\")\n",
    "    print(f\"  Std:  {np.std(negative_similarities):.4f}\")\n",
    "    print(f\"  Range: [{np.min(negative_similarities):.4f}, {np.max(negative_similarities):.4f}]\")\n",
    "    print(f\"\\nOptimal Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'positive_similarities': positive_similarities,\n",
    "        'negative_similarities': negative_similarities,\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'stats_df': stats_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d994ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_similarity_analysis(similarity_data, num_examples=5):\n",
    "    \"\"\"Detailed analysis with specific examples\"\"\"\n",
    "    \n",
    "    similarity_matrix = similarity_data['similarity_matrix']\n",
    "    label_test = similarity_data['label_test']\n",
    "    label_org = similarity_data['label_org']\n",
    "    paths_test = similarity_data['paths_test']\n",
    "    paths_org = similarity_data['paths_org']\n",
    "    refer_dict = similarity_data['refer_dict']\n",
    "    \n",
    "    print(\"DETAILED SIMILARITY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Show top similarities for each test image\n",
    "    for i in range(min(num_examples, len(label_test))):\n",
    "        test_label = label_test[i]\n",
    "        test_path = paths_test[i]\n",
    "        \n",
    "        # Get similarities for this test image\n",
    "        similarities = similarity_matrix[i]\n",
    "        \n",
    "        # Get top 3 most similar reference images\n",
    "        top_indices = np.argsort(similarities)[::-1][:3]\n",
    "        \n",
    "        print(f\"\\nTest Image {i+1}: {os.path.basename(test_path)}\")\n",
    "        print(f\"True Class: {refer_dict[test_label]}\")\n",
    "        print(\"Top 3 Most Similar Reference Images:\")\n",
    "        \n",
    "        for rank, ref_idx in enumerate(top_indices):\n",
    "            ref_label = label_org[ref_idx]\n",
    "            ref_path = paths_org[ref_idx]\n",
    "            similarity_score = similarities[ref_idx]\n",
    "            \n",
    "            match_status = \"‚úì MATCH\" if ref_label == test_label else \"‚úó NO MATCH\"\n",
    "            \n",
    "            print(f\"  {rank+1}. {os.path.basename(ref_path)}\")\n",
    "            print(f\"     Class: {refer_dict[ref_label]}\")\n",
    "            print(f\"     Similarity: {similarity_score:.4f} {match_status}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c602474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_similarity_results(similarity_data, analysis_results, output_dir=\"similarity_analysis\"):\n",
    "    \"\"\"Save analysis results to files\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save similarity matrix\n",
    "    np.save(os.path.join(output_dir, 'similarity_matrix.npy'), \n",
    "            similarity_data['similarity_matrix'])\n",
    "    \n",
    "    # Save detailed results\n",
    "    results = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'num_test_samples': len(similarity_data['label_test']),\n",
    "        'num_reference_samples': len(similarity_data['label_org']),\n",
    "        'num_classes': len(set(similarity_data['label_test'])),\n",
    "        'best_threshold': float(analysis_results['best_threshold']),\n",
    "        'best_accuracy': float(analysis_results['best_accuracy']),\n",
    "        'positive_sim_mean': float(np.mean(analysis_results['positive_similarities'])),\n",
    "        'positive_sim_std': float(np.std(analysis_results['positive_similarities'])),\n",
    "        'negative_sim_mean': float(np.mean(analysis_results['negative_similarities'])),\n",
    "        'negative_sim_std': float(np.std(analysis_results['negative_similarities']))\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(os.path.join(output_dir, 'similarity_analysis_results.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save statistics DataFrame\n",
    "    analysis_results['stats_df'].to_csv(\n",
    "        os.path.join(output_dir, 'similarity_statistics.csv'), index=False)\n",
    "    \n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def run_similarity_analysis(test_path, model, transforms, key=\"000000\"):\n",
    "    \"\"\"Run complete similarity analysis\"\"\"\n",
    "    \n",
    "    print(\"Starting Cosine Similarity Analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Calculate similarity matrix\n",
    "    print(\"Step 1: Calculating similarity matrix...\")\n",
    "    similarity_data = calculate_cosine_similarity_matrix(\n",
    "        test_path, model, transforms, key=key, max_samples=100\n",
    "    )\n",
    "    \n",
    "    if similarity_data is None:\n",
    "        print(\"Failed to calculate similarities!\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Visualize results\n",
    "    print(\"\\nStep 2: Generating visualizations...\")\n",
    "    analysis_results = visualize_similarity_distribution(similarity_data)\n",
    "    \n",
    "    # Step 3: Detailed analysis\n",
    "    print(\"\\nStep 3: Detailed similarity analysis...\")\n",
    "    detailed_similarity_analysis(similarity_data, num_examples=5)\n",
    "    \n",
    "    # Step 4: Save results\n",
    "    print(\"\\nStep 4: Saving results...\")\n",
    "    saved_results = save_similarity_results(similarity_data, analysis_results)\n",
    "    \n",
    "    return {\n",
    "        'similarity_data': similarity_data,\n",
    "        'analysis_results': analysis_results,\n",
    "        'saved_results': saved_results\n",
    "    }\n",
    "\n",
    "# Execute analysis\n",
    "test_path = \"logo_verify_test\"  # Thay ƒë·ªïi path theo d·ªØ li·ªáu c·ªßa b·∫°n\n",
    "\n",
    "if os.path.exists(test_path):\n",
    "    results = run_similarity_analysis(test_path, model, preprocess, key=\"000000\")\n",
    "else:\n",
    "    print(f\"Test path not found: {test_path}\")\n",
    "    print(\"Please update the path to your test data.\")\n",
    "    \n",
    "    # Demo v·ªõi synthetic data n·∫øu kh√¥ng c√≥ real data\n",
    "    print(\"\\nCreating demo with synthetic data...\")\n",
    "    \n",
    "    # T·∫°o fake embeddings ƒë·ªÉ demo\n",
    "    num_classes = 5\n",
    "    samples_per_class = 10\n",
    "    embedding_dim = 256\n",
    "    \n",
    "    fake_embeddings_ref = []\n",
    "    fake_embeddings_test = []\n",
    "    fake_labels_ref = []\n",
    "    fake_labels_test = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # T·∫°o center embedding cho m·ªói class\n",
    "        center = np.random.randn(embedding_dim)\n",
    "        \n",
    "        # Reference embeddings (g·∫ßn center)\n",
    "        for _ in range(samples_per_class//2):\n",
    "            noise = np.random.randn(embedding_dim) * 0.1\n",
    "            fake_embeddings_ref.append(center + noise)\n",
    "            fake_labels_ref.append(class_id)\n",
    "        \n",
    "        # Test embeddings (xa center h∆°n m·ªôt ch√∫t)\n",
    "        for _ in range(samples_per_class//2):\n",
    "            noise = np.random.randn(embedding_dim) * 0.2\n",
    "            fake_embeddings_test.append(center + noise)\n",
    "            fake_labels_test.append(class_id)\n",
    "    \n",
    "    fake_embeddings_ref = np.vstack(fake_embeddings_ref)\n",
    "    fake_embeddings_test = np.vstack(fake_embeddings_test)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    fake_embeddings_ref = fake_embeddings_ref / np.linalg.norm(fake_embeddings_ref, axis=1, keepdims=True)\n",
    "    fake_embeddings_test = fake_embeddings_test / np.linalg.norm(fake_embeddings_test, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    fake_similarity_matrix = cosine_similarity(fake_embeddings_test, fake_embeddings_ref)\n",
    "    \n",
    "    # Create fake data structure\n",
    "    fake_similarity_data = {\n",
    "        'similarity_matrix': fake_similarity_matrix,\n",
    "        'embeddings_org': fake_embeddings_ref,\n",
    "        'embeddings_test': fake_embeddings_test,\n",
    "        'label_org': fake_labels_ref,\n",
    "        'label_test': fake_labels_test,\n",
    "        'paths_org': [f\"ref_class_{l}_{i}.jpg\" for i, l in enumerate(fake_labels_ref)],\n",
    "        'paths_test': [f\"test_class_{l}_{i}.jpg\" for i, l in enumerate(fake_labels_test)],\n",
    "        'refer_dict': {i: f\"Logo_Class_{i}\" for i in range(num_classes)}\n",
    "    }\n",
    "    \n",
    "    print(\"Analyzing synthetic data...\")\n",
    "    fake_analysis_results = visualize_similarity_distribution(fake_similarity_data)\n",
    "    detailed_similarity_analysis(fake_similarity_data, num_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dbeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced preprocessing and embedding extraction\n",
    "preprocess = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225], \n",
    "        max_pixel_value=255.0, \n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], p=1.)\n",
    "\n",
    "def extract_embedding(image_path, model, transforms):\n",
    "    \"\"\"Extract embedding vector from image with error handling\"\"\"\n",
    "    try:\n",
    "        # Load and convert image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Apply transforms\n",
    "        transformed = transforms(image=image_array)[\"image\"]\n",
    "        \n",
    "        # Extract embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = model(transformed.unsqueeze(0).to(device))\n",
    "            # Normalize embedding\n",
    "            embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding.cpu().numpy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(image_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_embeddings_batch(image_paths, model, transforms, batch_size=32):\n",
    "    \"\"\"Extract embeddings in batches for better efficiency\"\"\"\n",
    "    embeddings = []\n",
    "    valid_paths = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_valid_paths = []\n",
    "            \n",
    "            # Load batch of images\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    image = Image.open(path).convert('RGB')\n",
    "                    image_array = np.array(image)\n",
    "                    transformed = transforms(image=image_array)[\"image\"]\n",
    "                    batch_images.append(transformed)\n",
    "                    batch_valid_paths.append(path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {os.path.basename(path)}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if batch_images:\n",
    "                # Process batch\n",
    "                batch_tensor = torch.stack(batch_images).to(device)\n",
    "                batch_embeddings = model(batch_tensor)\n",
    "                # Normalize embeddings\n",
    "                batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "                \n",
    "                embeddings.extend(batch_embeddings.cpu().numpy())\n",
    "                valid_paths.extend(batch_valid_paths)\n",
    "    \n",
    "    return embeddings, valid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0207a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution with comprehensive testing\n",
    "def run_complete_similarity_analysis(test_paths, model, transforms, key=\"000000\"):\n",
    "    \"\"\"Run complete similarity analysis with multiple fallback paths\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Logo Similarity Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Try multiple test paths\n",
    "    test_successful = False\n",
    "    similarity_data = None\n",
    "    \n",
    "    for test_path in test_paths:\n",
    "        if os.path.exists(test_path):\n",
    "            print(f\"üîç Testing with: {test_path}\")\n",
    "            try:\n",
    "                similarity_data = calculate_cosine_similarity_matrix_enhanced(\n",
    "                    test_path, model, transforms, key=key, max_samples=100\n",
    "                )\n",
    "                \n",
    "                if similarity_data is not None:\n",
    "                    test_successful = True\n",
    "                    print(f\"‚úÖ Successfully processed data from: {test_path}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to process {test_path}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"üìÅ Path not found: {test_path}\")\n",
    "    \n",
    "    if not test_successful or similarity_data is None:\n",
    "        print(\"‚ö†Ô∏è No real data found. Creating synthetic demo...\")\n",
    "        similarity_data = create_synthetic_demo()\n",
    "    \n",
    "    if similarity_data is None:\n",
    "        print(\"‚ùå Failed to create any data for analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Run analysis\n",
    "    print(\"\\nüìä Generating visualizations...\")\n",
    "    try:\n",
    "        analysis_results = visualize_similarity_distribution(similarity_data)\n",
    "        \n",
    "        print(\"\\nüîç Detailed analysis...\")\n",
    "        detailed_similarity_analysis(similarity_data, num_examples=5)\n",
    "        \n",
    "        print(\"\\nüíæ Saving results...\")\n",
    "        saved_results = save_similarity_results(similarity_data, analysis_results)\n",
    "        \n",
    "        return {\n",
    "            'similarity_data': similarity_data,\n",
    "            'analysis_results': analysis_results,\n",
    "            'saved_results': saved_results,\n",
    "            'model_info': model_info\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in analysis: {e}\")\n",
    "        return {'similarity_data': similarity_data, 'model_info': model_info}\n",
    "\n",
    "def create_synthetic_demo():\n",
    "    \"\"\"Create synthetic data for demonstration\"\"\"\n",
    "    print(\"üé≠ Creating synthetic logo similarity demo...\")\n",
    "    \n",
    "    num_classes = 10\n",
    "    samples_per_class = 20\n",
    "    embedding_dim = 256\n",
    "    \n",
    "    fake_embeddings_ref = []\n",
    "    fake_embeddings_test = []\n",
    "    fake_labels_ref = []\n",
    "    fake_labels_test = []\n",
    "    \n",
    "    # Create realistic logo class names\n",
    "    logo_names = ['Amazon', 'Apple', 'Google', 'Microsoft', 'Nike', \n",
    "                 'Adidas', 'Coca-Cola', 'Pepsi', 'McDonald', 'Starbucks']\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # Create center embedding for each logo class\n",
    "        center = np.random.randn(embedding_dim)\n",
    "        center = center / np.linalg.norm(center)  # Normalize\n",
    "        \n",
    "        # Reference embeddings (close to center)\n",
    "        for i in range(samples_per_class//2):\n",
    "            noise = np.random.randn(embedding_dim) * 0.05  # Small noise\n",
    "            embedding = center + noise\n",
    "            embedding = embedding / np.linalg.norm(embedding)  # Normalize\n",
    "            fake_embeddings_ref.append(embedding)\n",
    "            fake_labels_ref.append(class_id)\n",
    "        \n",
    "        # Test embeddings (slightly more variation)\n",
    "        for i in range(samples_per_class//2):\n",
    "            noise = np.random.randn(embedding_dim) * 0.15  # More noise\n",
    "            embedding = center + noise\n",
    "            embedding = embedding / np.linalg.norm(embedding)  # Normalize\n",
    "            fake_embeddings_test.append(embedding)\n",
    "            fake_labels_test.append(class_id)\n",
    "    \n",
    "    fake_embeddings_ref = np.vstack(fake_embeddings_ref)\n",
    "    fake_embeddings_test = np.vstack(fake_embeddings_test)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    fake_similarity_matrix = cosine_similarity(fake_embeddings_test, fake_embeddings_ref)\n",
    "    \n",
    "    # Create fake data structure\n",
    "    fake_similarity_data = {\n",
    "        'similarity_matrix': fake_similarity_matrix,\n",
    "        'embeddings_org': fake_embeddings_ref,\n",
    "        'embeddings_test': fake_embeddings_test,\n",
    "        'label_org': fake_labels_ref,\n",
    "        'label_test': fake_labels_test,\n",
    "        'paths_org': [f\"ref_{logo_names[l]}_{i:03d}.jpg\" for i, l in enumerate(fake_labels_ref)],\n",
    "        'paths_test': [f\"test_{logo_names[l]}_{i:03d}.jpg\" for i, l in enumerate(fake_labels_test)],\n",
    "        'refer_dict': {i: logo_names[i] for i in range(num_classes)}\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Created synthetic dataset:\")\n",
    "    print(f\"   Classes: {num_classes}\")\n",
    "    print(f\"   Reference samples: {len(fake_labels_ref)}\")\n",
    "    print(f\"   Test samples: {len(fake_labels_test)}\")\n",
    "    \n",
    "    return fake_similarity_data\n",
    "\n",
    "# Test paths to try\n",
    "test_paths = [\n",
    "    \"logo_verify_test\",\n",
    "    \"../logo_verify_test\", \n",
    "    \"../../logo_verify_test\",\n",
    "    \"data/logo_verify_test\",\n",
    "    \"../data/logo_verify_test\"\n",
    "]\n",
    "\n",
    "print(f\"üèÅ Starting analysis with {'trained' if model_loaded else 'random'} model\")\n",
    "print(f\"Model info: {model_info}\")\n",
    "\n",
    "# Run the complete analysis\n",
    "results = run_complete_similarity_analysis(test_paths, model, preprocess, key=\"000000\")\n",
    "\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    if 'analysis_results' in results:\n",
    "        analysis = results['analysis_results']\n",
    "        print(f\"üìà Best Accuracy: {analysis.get('best_accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"üéØ Optimal Threshold: {analysis.get('best_threshold', 'N/A'):.4f}\")\n",
    "    print(f\"ü§ñ Model Status: {'Trained' if model_loaded else 'Random Initialization'}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå Analysis failed completely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab956b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different model architectures\n",
    "def test_multiple_models():\n",
    "    \"\"\"Test similarity analysis with different model architectures\"\"\"\n",
    "    \n",
    "    model_configs = [\n",
    "        {'backbone': 'mobilenet_v2', 'emb_dim': 256, 'paths': ['SupConLoss_BBMobileNetV2.pth']},\n",
    "        {'backbone': 'vgg16', 'emb_dim': 256, 'paths': ['SupConLoss_BBVGG16.pth']},\n",
    "        {'backbone': 'resnet18', 'emb_dim': 256, 'paths': ['SupConLoss_BBResNet18.pth']},\n",
    "        {'backbone': 'resnet50', 'emb_dim': 256, 'paths': ['SupConLoss_BBResNet50.pth']}\n",
    "    ]\n",
    "    \n",
    "    results_comparison = {}\n",
    "    \n",
    "    for config in model_configs:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üß† Testing {config['backbone'].upper()} Architecture\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Try loading model\n",
    "        model_paths = [f\"Model2/{path}\" for path in config['paths']] + config['paths']\n",
    "        test_model, loaded, info = load_trained_model(\n",
    "            model_paths, \n",
    "            backbone=config['backbone'], \n",
    "            emb_dim=config['emb_dim']\n",
    "        )\n",
    "        \n",
    "        if loaded:\n",
    "            # Quick test with synthetic data\n",
    "            print(\"üé≠ Quick test with synthetic data...\")\n",
    "            synthetic_data = create_synthetic_demo()\n",
    "            \n",
    "            if synthetic_data:\n",
    "                analysis = visualize_similarity_distribution(synthetic_data)\n",
    "                results_comparison[config['backbone']] = {\n",
    "                    'loaded': True,\n",
    "                    'accuracy': analysis.get('best_accuracy', 0.0),\n",
    "                    'threshold': analysis.get('best_threshold', 0.0),\n",
    "                    'model_info': info\n",
    "                }\n",
    "            else:\n",
    "                results_comparison[config['backbone']] = {'loaded': True, 'error': 'Failed synthetic test'}\n",
    "        else:\n",
    "            results_comparison[config['backbone']] = {'loaded': False, 'model_info': info}\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for backbone, result in results_comparison.items():\n",
    "        status = \"‚úÖ Loaded\" if result.get('loaded', False) else \"‚ùå Failed\"\n",
    "        accuracy = result.get('accuracy', 'N/A')\n",
    "        print(f\"{backbone.upper():<15} | {status:<10} | Accuracy: {accuracy}\")\n",
    "    \n",
    "    return results_comparison\n",
    "\n",
    "# Uncomment to test multiple models\n",
    "# model_comparison = test_multiple_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae8eeb",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### 1. **Model Loading**\n",
    "The notebook automatically tries to load trained models from multiple locations:\n",
    "- `SupConLoss_BBMobileNetV2.pth`\n",
    "- `Model2/SupConLoss_BBMobileNetV2.pth`\n",
    "- `outputs/model_best.pth`\n",
    "\n",
    "### 2. **Data Paths**\n",
    "Update the `test_paths` list with your actual data locations:\n",
    "```python\n",
    "test_paths = [\n",
    "    \"your/actual/path/to/logo_verify_test\",\n",
    "    \"another/possible/path\"\n",
    "]\n",
    "```\n",
    "\n",
    "### 3. **Key Parameter**\n",
    "The `key=\"000000\"` parameter distinguishes reference vs test images:\n",
    "- Images containing \"000000\" = Reference images\n",
    "- Images NOT containing \"000000\" = Test images\n",
    "\n",
    "### 4. **Output**\n",
    "The analysis generates:\n",
    "- Similarity distribution visualizations\n",
    "- Accuracy vs threshold plots\n",
    "- Detailed similarity statistics\n",
    "- Saved results in `similarity_analysis/` folder\n",
    "\n",
    "### 5. **Troubleshooting**\n",
    "- If no real data is found, synthetic demo data is created\n",
    "- If sklearn fails, manual cosine similarity is used\n",
    "- Model loading has multiple fallback options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
