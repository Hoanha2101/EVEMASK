{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8365f95",
   "metadata": {},
   "source": [
    "# Cài đặt thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-metric-learning -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8dd76",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda import amp\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621420e8",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fb33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f741f",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (224, 224)\n",
    "data_transform = A.Compose([A.Resize(INPUT_SIZE[0], INPUT_SIZE[1]),\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.VerticalFlip(p=0.5),\n",
    "                        A.Rotate(limit=45, p=1.0),\n",
    "                        A.CoarseDropout(\n",
    "                                    max_holes=8,                \n",
    "                                    max_height=16,              \n",
    "                                    max_width=16,               \n",
    "                                    min_holes=1,                \n",
    "                                    min_height=8,               \n",
    "                                    min_width=8,                \n",
    "                                    fill_value=0,               \n",
    "                                    p=0.5                        \n",
    "                                ),\n",
    "                        A.RandomBrightnessContrast(\n",
    "                                brightness_limit=(-0.1,0.1), \n",
    "                                contrast_limit=(-0.1, 0.1), \n",
    "                                p=0.5),\n",
    "                        A.Normalize(\n",
    "                                mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225], \n",
    "                                max_pixel_value=255.0, \n",
    "                                p=1.0),\n",
    "                        ToTensorV2()], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dcb4a7",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA(Dataset):\n",
    "  def __init__(self, path, transform = None, phase = \"train\"):\n",
    "    self.path = path\n",
    "    self.phase = phase\n",
    "    self.transform = transform\n",
    "\n",
    "    folders = os.listdir(path)\n",
    "    LEN = 0\n",
    "    IMAGE = []\n",
    "    LABEL = []\n",
    "\n",
    "    NOTE_LABEL = {}\n",
    "    for i, value in enumerate(folders):\n",
    "      NOTE_LABEL[value] = i\n",
    "    print(NOTE_LABEL)\n",
    "\n",
    "    for image_folder in folders:\n",
    "      items_path = os.path.join(self.path, image_folder)\n",
    "      items_list = os.listdir(items_path)\n",
    "\n",
    "      LEN = LEN + len(items_list)\n",
    "      for image_name in items_list:\n",
    "        image_path = os.path.join(items_path, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            IMAGE.append(image)\n",
    "            LABEL.append(NOTE_LABEL[image_folder])\n",
    "\n",
    "    self.LEN = len(IMAGE)\n",
    "    self.IMAGE = IMAGE\n",
    "    self.LABEL = LABEL\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.LEN\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    anchor_img = self.IMAGE[idx]\n",
    "    anchor_label = self.LABEL[idx]\n",
    "    \n",
    "    if self.phase == \"train\":\n",
    "        \n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(image=anchor_img)[\"image\"]\n",
    "            \n",
    "        return anchor_img, torch.tensor(anchor_label, dtype=torch.float32)\n",
    "    else:\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(image=anchor_img)[\"image\"]\n",
    "        return anchor_img, torch.tensor(anchor_label, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "train_path = \"/kaggle/input/model2-94logo/data_classification\"\n",
    "train_data =  DATA(train_path, data_transform, phase = \"train\")\n",
    "end = time.time()\n",
    "print(f\"Load time: {round(end - start, 4)} s\")\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff37c2",
   "metadata": {},
   "source": [
    "### Show check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_img, label = train_data[100] # image at index = 100\n",
    "img_np = anchor_img.numpy()\n",
    "img_np = np.transpose(img_np, (1,2,0))\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  32 # Batch = 32 là max khi train với colab và kaggle, nếu lớn hơn thì out of memory -- Vram có 16gb thôi\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers = os.cpu_count()\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ba934",
   "metadata": {},
   "source": [
    "# SupCon Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080369c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.loss_fn = losses.ContrastiveLoss(temperature=self.temperature)\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        return self.loss_fn(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, temperature=0.2):\n",
    "#         super(SupervisedContrastiveLoss, self).__init__()\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def forward(self, feature_vectors, labels):\n",
    "#         # Normalize feature vectors\n",
    "#         feature_vectors_normalized = F.normalize(feature_vectors, p=2, dim=1)\n",
    "#         # Compute logits\n",
    "#         logits = torch.div(\n",
    "#             torch.matmul(\n",
    "#                 feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "#             ),\n",
    "#             self.temperature,\n",
    "#         )\n",
    "#         return losses.NTXentLoss(temperature=0.07)(logits, torch.squeeze(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df35d7",
   "metadata": {},
   "source": [
    "# BackBone - emb_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f25419",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e8577",
   "metadata": {},
   "source": [
    "##### #Custom FC head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec293609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_ = models.vgg16(pretrained=True)\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv = model_.features\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb388f",
   "metadata": {},
   "source": [
    "##### #Get Full Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ = models.vgg16(pretrained=True)\n",
    "\n",
    "# class Network(nn.Module):\n",
    "#     def __init__(self, emb_dim=128):\n",
    "#         super(Network, self).__init__()\n",
    "#         self.conv = model_.features\n",
    "        \n",
    "#         # Full head của VGG16 gồm 3 Linear layers\n",
    "#         self.base_classifier = model_.classifier  # Gồm cả 1000 lớp output\n",
    "\n",
    "#         # Sau lớp 1000-d, thêm PReLU và chuyển về emb_dim\n",
    "#         self.tail = nn.Sequential(\n",
    "#             nn.PReLU(),\n",
    "#             nn.Linear(1000, emb_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.base_classifier(x)\n",
    "#         x = self.tail(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84389619",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1a643",
   "metadata": {},
   "source": [
    "##### #Custom FC head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = models.vgg19(pretrained=True)\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv = model_.features\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce3feb",
   "metadata": {},
   "source": [
    "##### #Get Full Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ = models.vgg19(pretrained=True)\n",
    "\n",
    "# class Network(nn.Module):\n",
    "#     def __init__(self, emb_dim=128):\n",
    "#         super(Network, self).__init__()\n",
    "#         self.conv = model_.features\n",
    "        \n",
    "#         self.base_classifier = model_.classifier\n",
    "\n",
    "#         self.tail = nn.Sequential(\n",
    "#             nn.PReLU(),\n",
    "#             nn.Linear(1000, emb_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.base_classifier(x)\n",
    "#         x = self.tail(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c485727",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        base_model = models.resnet18(pretrained=True)  \n",
    "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])  \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)           # [B, 512, 1, 1]\n",
    "        x = torch.flatten(x, 1)        # [B, 512]\n",
    "        x = self.fc(x)                 # [B, emb_dim]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e866f1",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Bỏ layer cuối cùng (fc)\n",
    "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # Output: [B, 2048, 1, 1]\n",
    "\n",
    "        # FC Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)           # [B, 2048, 1, 1]\n",
    "        x = torch.flatten(x, 1)        # [B, 2048]\n",
    "        x = self.fc(x)                 # [B, emb_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530738cb",
   "metadata": {},
   "source": [
    "### MobileNetV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0685243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        base_model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        # Bỏ classifier, giữ lại feature extractor\n",
    "        self.backbone = base_model.features  # Output: [B, 1280, 7, 7]\n",
    "\n",
    "        # Global Average Pooling + FC head\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Output: [B, 1280, 1, 1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)           # [B, 1280, 7, 7]\n",
    "        x = self.pool(x)               # [B, 1280, 1, 1]\n",
    "        x = torch.flatten(x, 1)        # [B, 1280]\n",
    "        x = self.fc(x)                 # [B, emb_dim]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcead7e",
   "metadata": {},
   "source": [
    "# MobileNetv3 - Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25613ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        base_model = models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "        # Dùng phần features của model (output: [B, 576, 7, 7])\n",
    "        self.backbone = base_model.features\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Output: [B, 576, 1, 1]\n",
    "\n",
    "        # FC Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(576, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)   # [B, 576, 7, 7]\n",
    "        x = self.pool(x)       # [B, 576, 1, 1]\n",
    "        x = torch.flatten(x, 1)  # [B, 576]\n",
    "        x = self.fc(x)         # [B, emb_dim]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dac640",
   "metadata": {},
   "source": [
    "# MobileNetv3 - Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b389d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        base_model = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "        # Lấy phần feature extractor\n",
    "        self.backbone = base_model.features  # Output: [B, 960, 7, 7]\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # [B, 960, 1, 1]\n",
    "\n",
    "        # FC head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(960, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)   # [B, 960, 7, 7]\n",
    "        x = self.pool(x)       # [B, 960, 1, 1]\n",
    "        x = torch.flatten(x, 1)  # [B, 960]\n",
    "        x = self.fc(x)         # [B, emb_dim]\n",
    "        return x\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f77662",
   "metadata": {},
   "source": [
    "# Test model forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(256).to(device)\n",
    "x = torch.rand([32, 3, 224, 224]).to(device) # input random\n",
    "output = model(x)\n",
    "print(output.shape) # output is torch.Size([32, 256]) -> good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044013c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18741e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 256 #\n",
    "model = Network(embedding_dims).to(device)\n",
    "criterion = SupervisedContrastiveLoss(temperature=0.1).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647987d8",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "def TEST(folder_path, model, transforms, key):\n",
    "\n",
    "    label_org = []\n",
    "    dir_org = []\n",
    "    label_test = []\n",
    "    dir_test_path = []\n",
    "    dir_org_path = []\n",
    "    REFER_DICT = {}\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over each subfolder in the folder_path\n",
    "        for label_index, subfolder_name in enumerate(os.listdir(folder_path)):\n",
    "            REFER_DICT[label_index] = subfolder_name\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            image_files = os.listdir(subfolder_path)\n",
    "            for image_index, image_file in enumerate(image_files):\n",
    "\n",
    "                image_path = os.path.join(subfolder_path, image_file)\n",
    "\n",
    "                if key in image_path:\n",
    "\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    image = transforms(image=np.array(image))[\"image\"]\n",
    "                    # Extract the embedding for the first image in the folder\n",
    "                    embedding = model(image.unsqueeze(0).to(\"cuda\"))\n",
    "                    dir_org.append(embedding)\n",
    "                    label_org.append(label_index)\n",
    "                    dir_org_path.append(image_path)\n",
    "                else:\n",
    "                    # Store the path and label for other images\n",
    "                    dir_test_path.append(image_path)\n",
    "                    label_test.append(label_index)\n",
    "\n",
    "        predict_label = []\n",
    "        Max_sim = []\n",
    "\n",
    "        # Iterate over test images\n",
    "        for test_image_path in dir_test_path:\n",
    "\n",
    "            test_image = Image.open(test_image_path).convert('RGB')\n",
    "            test_image = transforms(image=np.array(test_image))[\"image\"]\n",
    "\n",
    "            # Extract the embedding for the test image\n",
    "            test_embedding = model(test_image.unsqueeze(0).to(\"cuda\"))\n",
    "            similarities = []\n",
    "\n",
    "            # Calculate cosine similarity with each original embedding\n",
    "            for org_embedding in dir_org:\n",
    "\n",
    "                cosine_sim = cosine_similarity(org_embedding.cpu().detach().numpy(), test_embedding.cpu().detach().numpy())\n",
    "                similarities.append(cosine_sim[0][0])\n",
    "            Max_sim.append(max(similarities))\n",
    "            max_similarity_index = np.argmax(similarities)\n",
    "\n",
    "            predict_label.append(label_org[max_similarity_index])\n",
    "\n",
    "        accuracy = accuracy_score(predict_label, label_test)\n",
    "\n",
    "\n",
    "        \n",
    "        print(f'----Accuracy: {accuracy:.4f}')\n",
    "        print()\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "preprocess = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed4ec5",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "model.train()\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "ACC = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor_img, label) in enumerate(train_loader):\n",
    "        anchor_img = anchor_img.to(device).float()\n",
    "        label = label.to(device)\n",
    "        with torch.amp.autocast('cuda',enabled=True):\n",
    "            outputs = model(anchor_img)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss = loss / 4\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % 4 == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()      \n",
    "            \n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        test_path = \"/kaggle/input/test-set-seg-extract/TEST_SET\" # Đổi địa chỉ của cái này\n",
    "        accuracy = TEST(test_path, model, preprocess, key = \"000000\")\n",
    "        \n",
    "            \n",
    "        model.train()\n",
    "        \n",
    "        if accuracy >= ACC:\n",
    "            pth = f\"/kaggle/working/model_best{accuracy*1000}.pth\"  # Đổi địa chỉ của cái này\n",
    "            torch.save(model, pth)\n",
    "            ACC = accuracy\n",
    "            \n",
    "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
